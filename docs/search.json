[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of the behavior of dairy cattle using R",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nWarning: Under Construction\n\n\n\n\n\nBefore you dive into the content of this guide, we feel it is our responsibility to inform you that it is currently under construction. We strongly recommend postponing your reading until we’ve completed the necessary updates to ensure your learning experience is as valuable as possible.\n\n\n\nWelcome to this online course on “Analysis of the behavior of dairy cattle using R”. This tutorial is divided into two parts, each of which plays a critical role in understanding and applying data-driven insights to the study of dairy cattle behavior, particularly in response to various human interventions, with a focus on enviromental conditions.\nPart 1, “Introduction to R,” provides the foundation for data analysis and visualization. In this section, we will introduce you to R, a powerful programming language and environment for statistical computing and graphics. We will explore the basics of R, from setting up the environment to performing data manipulation and visualization. These skills are essential for conducting the advanced statistical analysis presented in Part 2.\nPart 2, “Statistical Modeling of Dairy Cattle Behavior”, explores the realm of data-driven analysis concerning dairy cattle behavior. You will learn how to collect and preprocess behavioral data, and then apply advanced statistical modeling techniques to understand how climate conditions and other human interventions impact the behavior of dairy cattle.\nThis course is designed to provide elementary skills and knowledge necessary to conduct useful statistical analysis in the context of dairy cattle behavior. Whether you are a scientist, a farmer, a professional or a student, this course will provide you with the tools needed to make informed decisions and contribute to the welfare and productivity of dairy cattle."
  },
  {
    "objectID": "intro.html#the-r-language",
    "href": "intro.html#the-r-language",
    "title": "1  Introduction",
    "section": "1.1 The R Language",
    "text": "1.1 The R Language\nThe R programming language is a powerful tool designed for statistical computing and data analysis. Originating in the early 1990s at the University of Auckland, New Zealand, by Ross Ihaka and Robert Gentleman, R has garnered a dedicated following in the fields of data science, statistics, and data visualization.\nWhat sets R apart is its rich toolkit of statistical and graphical techniques, making it a top choice for researchers, statisticians, data analysts, and data scientists. R offers an extensive collection of packages and libraries tailored to specific data analysis and visualization needs, creating a diverse ecosystem of tools.\nSome important features of R Language:\n\nOpen Source Foundation: R is open-source, fostering a vibrant community of users and developers, driving its continuous improvement.\nData Manipulation Proficiency: R excels in data manipulation with libraries like dplyr and tidyr, simplifying data cleaning and reshaping.\nStatistical Versatility: R offers a wide range of statistical methods, from basic statistics to advanced modeling and hypothesis testing.\nElegant Data Visualization: R is renowned for its data visualization capabilities, with ggplot2 enabling customizable and appealing visualizations.\nExtensibility: R allows users to create and share packages, leading to a rich repository of specialized tools.\nReproducibility Support: R promotes reproducible research through scripting and documentation, enhancing research credibility.\nCommunity Support: R has a thriving community with abundant resources and forums for assistance and collaboration.\nCross-Platform Compatibility: R is available on Windows, macOS, and Linux, ensuring accessibility to a diverse user base."
  },
  {
    "objectID": "intro.html#tidyverse-a-revolution-in-r",
    "href": "intro.html#tidyverse-a-revolution-in-r",
    "title": "1  Introduction",
    "section": "1.2 Tidyverse: A Revolution in R",
    "text": "1.2 Tidyverse: A Revolution in R\nIn recent years, the emergence of tidyverse libraries has brought significant advancements to R, making data manipulation and analysis more straightforward. One notable innovation within the tidyverse is the introduction of pipe operations, marked by |&gt;, which streamline workflows and improve code readability.\nThe pipe operator allows you to chain data manipulation functions, effortlessly passing the output from one function to the next. This creates a more linear and intuitive sequence of operations, reducing the reliance on temporary variables and minimizing nested function calls. The result is code that is more comprehensible and easier to maintain.\nConsider a straightforward example of the pipe operator’s utility in R:\n\nlibrary(tidyverse)\n# Creating a sample data frame\ndata &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(28, 35, 22)\n)\n\n# Utilizing the pipe operator for data manipulation\nresult &lt;- data |&gt;\n  filter(Age &gt;= 25) |&gt;\n  arrange(desc(Age))\n\nprint(result)\n\n   Name Age\n1   Bob  35\n2 Alice  28\n\n\nIn this instance, we initially filter the rows where Age is greater than or equal to 25 and then arrange the data in descending order of Age. The pipe operator (|&gt;) effortlessly conveys the data frame from one operation to the next, rendering the code more succinct and comprehensible.\nThe pipe operator (|&gt;) elevates code readability by facilitating a natural and intuitive sequence of data manipulation operations. Let’s elucidate this with some examples.\nWithout the Pipe Operator:\nImagine you have a data frame containing a list of products, and you need to perform a series of data manipulations: filtering the products in stock, converting prices to a different currency, and calculating the average price of the remaining products. In the absence of the pipe operator, the code might resemble this:\n\n# Without the pipe operator\nin_stock_products &lt;- filter(products, stock_quantity &gt; 0)\nconverted_prices &lt;- mutate(in_stock_products, price_usd = price * exchange_rate)\naverage_price &lt;- mean(converted_prices$price_usd)\n\nIn this code, you must create intermediate variables (in_stock_products and converted_prices) to store the results at each step, which diminishes code readability.\nWith the Pipe Operator:\nNow, let’s rewrite the same task using the pipe operator:\n\naverage_price &lt;- products |&gt;\n  filter(stock_quantity &gt; 0) |&gt;\n  mutate(price_usd = price * exchange_rate) |&gt;\n  summarise(average_price = mean(price_usd))\n\nWith the pipe operator, the code unfolds as a coherent sequence, and you can execute operations on the data frame step by step, without the need for interim variables. This enhances code readability and facilitates comprehension. Each operation is distinct, and their order aligns with the logical flow of data manipulation.\nThe pipe operator also accommodates more intricate chains of operations, further enhancing code clarity. Here’s an extended example where we filter products, group them by category, calculate the average price for each category, and then sort the results:\n\nresult &lt;- products |&gt;\n  filter(stock_quantity &gt; 0) |&gt;\n  group_by(category) |&gt;\n  mutate(price_usd = price * exchange_rate) |&gt;\n  summarise(average_price = mean(price_usd)) |&gt;\n  arrange(desc(average_price))\n\nThe tidyverse represents a significant change in data analysis. Beyond simplifying data manipulation, it comprises a collection of packages designed to streamline data handling, analysis, and visualization. With tools like dplyr for efficient data manipulation and ggplot2 for effective data visualization, the tidyverse reshapes data analysis by providing a cohesive toolkit for modern data-driven tasks, benefiting data scientists and analysts."
  },
  {
    "objectID": "intro.html#navigating-the-tidyverse-in-r",
    "href": "intro.html#navigating-the-tidyverse-in-r",
    "title": "1  Introduction",
    "section": "1.3 Navigating the Tidyverse in R",
    "text": "1.3 Navigating the Tidyverse in R\nIn the following sections, we will dive into the practical application of R and the Tidyverse libraries. This guide aims to provide a hands-on demonstration of how to effectively use R and the Tidyverse for data manipulation and analysis.\nWe will explore a set of examples that illustrate the integration of Tidyverse libraries, such as dplyr for data manipulation and ggplot2 for data visualization, among others. These libraries have significantly improved the way data scientists and analysts work with data, streamlining the entire process and making it more accessible.\n\n1.3.1 Key Topics to Be Covered\n\nData Manipulation with dplyr: We will walk through the process of using dplyr functions like filter(), mutate(), select(), and more to efficiently filter, transform, and summarize data.\nTaming Data with tidyr: You will learn how to reshape and tidy your data with the tidyr package, making it easier to work with in downstream analyses.\nChaining Operations with the Pipe Operator (|&gt;): Building on the concept introduced earlier, we will explore how to use the pipe operator to create clear and efficient data manipulation workflows.\nGrouping and Aggregating Data: group_by() and summarize(): Discover how to group data by specific variables and then calculate summary statistics for those groups.\nData Visualization with ggplot2: We’ll demonstrate how to create stunning and informative data visualizations using the ggplot2 package, allowing you to convey your insights effectively.\n\nOur aim is to provide you with the foundational knowledge and practical skills required to begin your journey in R through the tidyverse libraries. Upon completing this material, you will emerge with a strong sense of confidence in your ability to proficiently conduct data analysis, visualization, and manipulation."
  },
  {
    "objectID": "start.html#installing-r-and-rstudio",
    "href": "start.html#installing-r-and-rstudio",
    "title": "2  Installation",
    "section": "2.1 Installing R and Rstudio",
    "text": "2.1 Installing R and Rstudio\nTo install R, go to the official R website: https://www.r-project.org/ and follow these steps based on your operating system:\n\n2.1.1 On Windows\n\nIn the left menu, select “CRAN.”\nChoose the “Download R for Windows” option.\nSelect the “base” option.\nOn the following page, choose the CRAN mirror closest to your location.\nDownload the latest version (e.g., “R-x.x.x for Windows”).\nDouble-click the downloaded file and follow the installation instructions.\n\n\n\n2.1.2 On Linux\n\nIn the left menu, select “CRAN.”\nChoose the “Download R for Linux” option.\nSelect the CRAN mirror closest to your location.\nChoose the link for your specific distribution.\nFollow the instructions provided on the page to install R.\n\n\n\n2.1.3 On macOS\n\nIn the left menu, select “CRAN.”\nChoose the “Download R for (Mac) OS X” option.\nSelect the CRAN mirror closest to your location.\nChoose the version you want (usually the latest version with a filename like “R-x.x.x.pkg”).\nDouble-click the downloaded file and follow the installation instructions."
  },
  {
    "objectID": "start.html#installing-rstudio",
    "href": "start.html#installing-rstudio",
    "title": "2  Installation",
    "section": "2.2 Installing Rstudio",
    "text": "2.2 Installing Rstudio\nTo install RStudio on Windows, follow these steps:\n\nVisit the RStudio downloads page by clicking on the link below:\nRStudio Downloads Page\nDownload the version that matches your operating system from the “All Installers” list.\nDouble-click the file you downloaded from the RStudio page and follow the installation instructions."
  },
  {
    "objectID": "start.html#integrated-development-environments-ides",
    "href": "start.html#integrated-development-environments-ides",
    "title": "2  Installation",
    "section": "2.3 Integrated Development Environments (IDEs)",
    "text": "2.3 Integrated Development Environments (IDEs)\n\n2.3.1 The Rstudio IDE\nIntegrated Development Environments (IDEs) play a crucial role in programming, providing an interactive and organized environment for developers. In the realm of the R programming language, one of the most popular IDEs is RStudio.\n\n\n\nA view of RStudio IDE\n\n\nRStudio is an open-source IDE specifically designed for R. It offers several key components:\n\nRStudio Interface:\n\nConsole: Execute R commands interactively.\nScript Editor: Write and edit R scripts with syntax highlighting and code suggestions.\nEnvironment and History: Monitor variables and functions, review command history.\nPlots and Visualizations: View interactive graphics generated from your code.\n\nProject Concept: RStudio encourages the use of projects to organize your data analysis or R development work. A project is an isolated workspace that contains all project-related files, scripts, data, and settings, enhancing organization and collaboration.\nR Scripts: R scripts are files containing R code for batch or interactive execution. Create and edit R scripts directly in RStudio, with integrated variable and object management.\nRData Files: RData files are native R data files used to save and load R objects (e.g., data frames, lists, variables) for reuse in R sessions.\n\nRStudio is a powerful and versatile IDE for R, catering to data scientists, analysts, and developers. Its user-friendly interface and integrated features enhance productivity and efficiency in data analysis and statistical programming.\n\n\n2.3.2 Cheat Sheets\nWhen it comes to learning more about RStudio and various R libraries, an invaluable resource at your disposal is the cheat sheets.\nIn RStudio, cheat sheets are available to provide concise summaries on how to utilize a range of packages, including insights into the functionality of RStudio itself. To access these cheat sheets, simply follow these steps:\n\nLaunch RStudio.\nNavigate to the “Help” menu located in the top menu bar of the window.\nWithin the “Help” menu, you’ll discover an option labeled “Cheat Sheets.” Click on this option.\nThis action will present you with a selection of available cheat sheets. Choose the one that is pertinent to the package or subject matter you wish to delve into.\nClick on the specific cheat sheet of your choice to open it. It will be displayed in a new browser tab or in a PDF viewer, depending on your RStudio’s configuration.\n\nThese cheat sheets contain succinct information and valuable insights on how to effectively utilize a variety of R features and specific packages. They serve as an excellent resource for swift reference and efficient learning.\n\n\n\nThe Rstudio cheat sheet"
  },
  {
    "objectID": "basics.html#syntax-object-types-and-structure",
    "href": "basics.html#syntax-object-types-and-structure",
    "title": "3  Basics",
    "section": "3.1 Syntax, object types and structure",
    "text": "3.1 Syntax, object types and structure\nIn R, you work with a variety of data types to represent and manipulate information. Understanding these data types is essential for effective data analysis. Let’s expand upon the data types in the context of the provided text.\n\n3.1.1 Character\nCharacter data types are used to represent text or strings. They are enclosed in single or double quotes and are ideal for working with labels, names, or textual data.\nExample:\n\n# Name of a dairy cow\ncow_name &lt;- \"Bessie\"\n\n\n\n3.1.2 Factor\nFactors are a unique data type in R, often used to represent categorical data. They have predefined levels, making them suitable for variables with distinct categories or levels.\nExample:\n\n# Cow breeds as a factor\ncow_breeds &lt;- factor(c(\"Holstein\", \"Jersey\", \"Guernsey\", \"Simmental\"))\n\n\n\n3.1.3 Double\nDouble data types represent numeric values, typically in decimal form. They are used for storing continuous, real-valued data, such as measurements and percentages.\nExample:\n\n# Milk production in gallons\nmilk_production_gallons &lt;- 15.5\n\n\n\n3.1.4 Integer\nInteger data types are used for whole numbers without decimal points. They are ideal for representing counts, quantities, or discrete values.\nExample:\n\n# Number of calves born\nnum_calves &lt;- 5L  # The 'L' indicates it's an integer\n\n\n\n3.1.5 Logical\nLogical data types, often referred to as booleans, represent binary values: TRUE or FALSE. They are essential for making logical comparisons and decisions in your code.\nExample:\n\n# Is the cow healthy?\nis_healthy &lt;- TRUE\n\n\n\n3.1.6 Date\nDate data types are used to work with dates and times. They allow you to represent specific points in time, including date and time together, in a structured manner.\nExample:\n\n# Date of last milk collection\nlast_milk_collection &lt;- as.Date(\"2023-10-25\")\n\n\n\n3.1.7 Lists\nLists are a versatile data type in R, capable of storing various objects of different types. They can contain scalars, vectors, data frames, and more.\nExample:\n\n# Information about a dairy farm as a list\nfarm_info &lt;- list(farm_name = \"Green Meadows\", location = \"Wisconsin\", cows = c(550, 600, 520, 580, 630), is_organic = FALSE)\n\nVarious types of data can be organized using a diverse range of data structures, as previously mentioned scalars, vectors, matrices, and data frames.\n\n\n3.1.8 Scalars\nScalars are single values in R. They can be integers, decimals, or characters. You might use scalars to represent individual data points.\nExample:\n\n# Number of dairy cows on a farm\nnum_cows &lt;- 100\n\n\n\n3.1.9 Vectors\nVectors are one-dimensional arrays that can store multiple values of the same data type. They are commonly used in R to work with collections of data. For example, you might use vectors to store information about individual cow weights.\nExample:\n\n# Weight of dairy cows in kilograms\ncow_weights &lt;- c(550, 600, 520, 580, 630)\n\n\n\n3.1.10 Matrices\nMatrices are two-dimensional arrays that can store data of the same data type. You might use matrices to represent data tables, like the weights of multiple cows over time.\nExample:\n\n# Create a matrix with cow weights for two weeks\ncow_weight_matrix &lt;- matrix(c(550, 600, 520, 580, 630, 560, 610, 530, 590, 640), nrow = 5, byrow = TRUE)\ncolnames(cow_weight_matrix) &lt;- c(\"Week1\", \"Week2\")\nrownames(cow_weight_matrix) &lt;- c(\"Cow1\", \"Cow2\", \"Cow3\", \"Cow4\", \"Cow5\")\n\n\n\n3.1.11 Data Frames\nData frames are a common data structure in R, designed for tabular data. They can store different types of data (e.g., scalars, vectors) in columns. You might use data frames to organize data like cow ID, age, weight, and milk production.\nExample:\n\n# Create a data frame with information about dairy cows\ndairy_cows &lt;- data.frame(\n  ID = c(1, 2, 3, 4, 5),\n  Name = c(\"Bessie\", \"MooMoo\", \"Daisy\", \"Buttercup\", \"Elsie\"),\n  Age = c(5, 4, 6, 3, 7),\n  Weight_kg = cow_weights,\n  Milk_Production_liters = c(20, 18, 22, 19, 21)\n)"
  },
  {
    "objectID": "basics.html#pre-compiled-functions",
    "href": "basics.html#pre-compiled-functions",
    "title": "3  Basics",
    "section": "3.2 Pre-compiled Functions",
    "text": "3.2 Pre-compiled Functions\nR provides pre-compiled functions and data structures that optimize the efficiency of working with these objects. Some of these functions have been demonstrated in the earlier examples, including:\n\nCombining elements: The c() function is fundamental in R for creating vectors by combining individual elements.\nMatrices: You can create matrices using the matrix() function to organize structured data.\nData Frames: The data.frame() function is employed to establish data frames for tabular data.\n\nHowever, there are numerous others, including:\n\nSummary Statistics: R offers pre-compiled functions like mean(), median(), and summary() for in-depth data analysis.\nPlotting: R provides functions such as plot() and hist() for crafting data visualizations.\n\nThese pre-compiled functions and data structures simplify the process of working with data, enabling efficient data manipulation and analysis.\nCertainly, here’s the revised version, including information about what “val” represents:"
  },
  {
    "objectID": "basics.html#understanding-the-generic-structure-of-an-r-function",
    "href": "basics.html#understanding-the-generic-structure-of-an-r-function",
    "title": "3  Basics",
    "section": "3.3 Understanding the Generic Structure of an R Function",
    "text": "3.3 Understanding the Generic Structure of an R Function\nIn R, every function adheres to a consistent and generic structure, typically defined as:\n\nfunction_name(arg1 = val, arg2 = val, arg3 = val, ..., argN = val)\n\n\nfunction_name: This is the name of the R function, and it signifies the specific operation that the function performs.\narg1, arg2, arg3, …, argN: These represent the function’s arguments, encapsulated within parentheses. Arguments are placeholders for the values or parameters that you provide to the function. The number and type of arguments may vary depending on the specific function.\n\nIn this context, “val” is a placeholder for the actual values or parameters you would provide when calling the function. The “val” should be replaced with specific values relevant to the function’s operation.\nExample:\n\n# Generic structure of a function\nresult &lt;- mean(x = c(1, 2, 3, NA), na.rm = T)"
  },
  {
    "objectID": "basics.html#dealing-with-na-values-in-r",
    "href": "basics.html#dealing-with-na-values-in-r",
    "title": "3  Basics",
    "section": "3.4 Dealing with “NA” Values in R",
    "text": "3.4 Dealing with “NA” Values in R\nIn R, “NA” represents missing or undefined data. It stands for “Not Available” and is used to indicate missing values. You may encounter different types of “NA” values, each tailored to specific data types. To handle missing data:\n\nDetect NAs with is.na() or complete.cases().\nRemove NAs using functions like na.omit().\nReplace NAs with specific values when needed.\nIgnore NAs in calculations using the na.rm parameter.\n\nExample:\n\n# Create a vector with missing data (NAs)\ndata_vector &lt;- c(12, NA, 25, 18, NA, 30)\n\n# Detect NAs in the vector\nna_indices &lt;- which(is.na(data_vector))\n\n# Remove NAs and create a new vector\nclean_vector &lt;- na.omit(data_vector)\n\n# Replace NAs with a specific value (e.g., 0)\ndata_vector[is.na(data_vector)] &lt;- 0\n\n# Calculate the mean of the cleaned vector, ignoring NAs\nmean_value &lt;- mean(clean_vector, na.rm = TRUE)\n\n# Print the results\ncat(\"Original Data Vector: \", data_vector, \"\\n\")\n\nOriginal Data Vector:  12 0 25 18 0 30 \n\ncat(\"Indices of NAs: \", na_indices, \"\\n\")\n\nIndices of NAs:  2 5 \n\ncat(\"Cleaned Data Vector: \", clean_vector, \"\\n\")\n\nCleaned Data Vector:  12 25 18 30 \n\ncat(\"Mean Value (Ignoring NAs): \", mean_value, \"\\n\")\n\nMean Value (Ignoring NAs):  21.25"
  },
  {
    "objectID": "basics.html#libraries",
    "href": "basics.html#libraries",
    "title": "3  Basics",
    "section": "3.5 Libraries",
    "text": "3.5 Libraries\nIn R, libraries are collections of functions and datasets that extend the capabilities of the base R system. They provide additional tools and functions for various tasks, such as data manipulation, visualization, and statistical analysis. Libraries are crucial for enhancing your data analysis and programming capabilities in R.\n\n3.5.1 Installing and Loading Libraries in R\nTo use a library in R, you first need to install it (usually only once) and then load it (each time you start a new R session) using the library() function. You can install and load libraries using RStudio or directly in R.\nExample: Installing and Loading the “tidyverse” Library in RStudio\n\nOpen RStudio.\nIn the RStudio console, you can install the “tidyverse” library using the install.packages() function:\n\n\ninstall.packages(\"tidyverse\")\n\nThis command will download and install the “tidyverse” package from the Comprehensive R Archive Network (CRAN).\n\nOnce the installation is complete, load the “tidyverse” library using the library() function:\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading the “tidyverse” library makes its functions and data manipulation tools available for your R session.\nNow you can take advantage of the powerful data analysis and visualization tools provided by the “tidyverse” package in your RStudio environment.\nLibraries like “tidyverse” are essential for extending R’s functionality and making it a powerful platform for data analysis and statistical computing."
  },
  {
    "objectID": "basics.html#data-import-in-r",
    "href": "basics.html#data-import-in-r",
    "title": "3  Basics",
    "section": "3.6 Data Import in R",
    "text": "3.6 Data Import in R\nData import is a crucial step in data analysis using R. R provides various functions and packages to import and manipulate data from different sources. Here, we’ll cover how to import pre-available data and work with commonly used file formats like CSV, TXT, and Excel.\n\n3.6.1 Importing Pre-Available Data\nIn R, there are built-in datasets that you can use for practice and analysis. One such dataset is the “mtcars” dataset.\n\n# Load the built-in \"mtcars\" dataset\ndata(mtcars)\n\n# View the first few rows of the dataset\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nThe “mtcars” dataset is now available for analysis.\n\n\n3.6.2 Importing Data from CSV\nSuppose you have data in a CSV (Comma-Separated Values) file. You can use the read_csv() function from the Tidyverse to import it. Here’s an example:\n\n# Import data from a CSV file\nmy_data &lt;- read_csv(\"my_data.csv\")\n\n# Display the first few rows of the imported data\nhead(my_data)\n\nReplace \"my_data.csv\" with the actual path to your CSV file.\n\n\n3.6.3 Importing Data from Text (TXT) Files\nTo import data from a plain text file, you can use the read_delim() function from Tidyverse. This function is versatile and can handle various delimiters, making it suitable for tab-delimited or space-delimited files. Here’s an example:\n\n# Import data from a text file\nmy_data &lt;- read_delim(\"my_data.txt\", delim = \"\\t\")\n\n# Display the first few rows of the imported data\nhead(my_data)\n\nReplace \"my_data.txt\" with the actual path to your text file and adjust the delim parameter as needed to match your file’s delimiter.\n\n\n3.6.4 Importing Data from Excel\nTo import data from Excel files, you can use the readxl package. First, install the package and then use the read_excel() function to import data from an Excel file.\nAssuming you have an Excel file named \"dairy_data.xlsx\" with a sheet named \"Cattle_Info\", you can import it as follows:\n\n# Install and load the \"readxl\" package (install only once)\n# install.packages(\"readxl\")\nlibrary(readxl)\n\n# Import data from an Excel file\ncattle_data_excel &lt;- read_excel(\"dairy_data.xlsx\", sheet = \"Cattle_Info\")\n\n# View the first few rows of the imported data\nhead(cattle_data_excel)\n\n\n\n3.6.5 Beyond Local Files\nBeyond local data storage, R enables you to seamlessly import datasets from the internet. This functionality provides access to a wide range of online data sources.\nTo import a dataset from the internet into R, you can employ the functions previously discussed, depending on the file format accessible via a URL. Let’s illustrate this with a practical example involving a .csv file related to milk production for dairy cows, hosted on GitHub. Here’s how to import this data into R:\n\n# Define the URL of the dataset\ndata_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/state_milk_production.csv\"\n\n\n# Import data from the internet\nstate_milk_production &lt;- read_csv(data_url, show_col_types = FALSE)\n\n# View the first few rows of the imported data\nhead(state_milk_production)\n\n# A tibble: 6 × 4\n  region    state          year milk_produced\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast Maine          1970     619000000\n2 Northeast New Hampshire  1970     356000000\n3 Northeast Vermont        1970    1970000000\n4 Northeast Massachusetts  1970     658000000\n5 Northeast Rhode Island   1970      75000000\n6 Northeast Connecticut    1970     661000000"
  },
  {
    "objectID": "manip.html#basic-data-manipulation-with-dplyr",
    "href": "manip.html#basic-data-manipulation-with-dplyr",
    "title": "4  Data manipulation",
    "section": "4.1 Basic Data Manipulation with dplyr",
    "text": "4.1 Basic Data Manipulation with dplyr\nWe’ll cover the fundamental data manipulation functions provided by the dplyr package in R such as: mutate() for adding or modifying variables, summarize() for calculating summary statistics ,group_by() and ungroup() for grouping and ungrouping data,filter() fro filtering rows, select() for selecting columns and arrange() for sorting data.\n\n\nmutate() - Adding or Modifying Variables\nThe mutate() function is used to create new variables or modify existing ones.\n\n# Adding a new variable: milk_per_cow_kg\nmutated_data &lt;- state_milk_production |&gt;\n  mutate(milk_produced_kg = milk_produced * 0.453592)  # Convert pounds to kilograms\n\nprint(mutated_data)\n\n# A tibble: 2,400 × 5\n   region    state          year milk_produced milk_produced_kg\n   &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1 Northeast Maine          1970     619000000        280773448\n 2 Northeast New Hampshire  1970     356000000        161478752\n 3 Northeast Vermont        1970    1970000000        893576240\n 4 Northeast Massachusetts  1970     658000000        298463536\n 5 Northeast Rhode Island   1970      75000000         34019400\n 6 Northeast Connecticut    1970     661000000        299824312\n 7 Northeast New York       1970   10341000000       4690594872\n 8 Northeast New Jersey     1970     730000000        331122160\n 9 Northeast Pennsylvania   1970    7124000000       3231389408\n10 Northeast Delaware       1970     130000000         58966960\n# ℹ 2,390 more rows\n\n\n\n\n\nsummarize() - Calculating Summary Statistics\nThe summarize() function calculates summary statistics for a specific group.\n\n# Calculate the total milk produced for each state\nsummary_data &lt;- state_milk_production |&gt;\n  group_by(state) |&gt;\n  summarize(total_milk_produced = sum(milk_produced))\nprint(summary_data)\n\n# A tibble: 50 × 2\n   state       total_milk_produced\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Alabama             20764000000\n 2 Alaska                675000000\n 3 Arizona            116937000000\n 4 Arkansas            26070000000\n 5 California        1219691000000\n 6 Colorado            86232000000\n 7 Connecticut         24574000000\n 8 Delaware             6094000000\n 9 Florida            106660000000\n10 Georgia             67745000000\n# ℹ 40 more rows\n\n\n\n\n\ngroup_by() - Grouping Data\nThe group_by() function is used to group data by a specific variable.\n\n# Group data by year and calculate the average milk produced\ngrouped_data &lt;- state_milk_production |&gt;\n  group_by(year) |&gt;\n  summarize(avg_milk_produced = mean(milk_produced))\n\nprint(grouped_data)\n\n# A tibble: 48 × 2\n    year avg_milk_produced\n   &lt;dbl&gt;             &lt;dbl&gt;\n 1  1970        2340140000\n 2  1971        2371320000\n 3  1972        2400500000\n 4  1973        2309820000\n 5  1974        2311720000\n 6  1975        2307960000\n 7  1976        2403600000\n 8  1977        2453080000\n 9  1978        2429220000\n10  1979        2467020000\n# ℹ 38 more rows\n\n\n\n\n\nfilter() - Filtering Rows\nThe filter() function is used to filter rows based on a condition.\n\n# Filter data for the year 2022\nfiltered_data &lt;- state_milk_production %&gt;%\n  filter(year == 2022)\n\nprint(filtered_data)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: region &lt;chr&gt;, state &lt;chr&gt;, year &lt;dbl&gt;, milk_produced &lt;dbl&gt;\n\n\n\n\n\nselect() - Selecting Columns\nThe select() function is used to select specific columns.\n\n# Select only the \"state\" and \"milk_produced\" columns\nselected_data &lt;- state_milk_production %&gt;%\n  select(state, milk_produced)\n\nprint(selected_data)\n\n# A tibble: 2,400 × 2\n   state         milk_produced\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Maine             619000000\n 2 New Hampshire     356000000\n 3 Vermont          1970000000\n 4 Massachusetts     658000000\n 5 Rhode Island       75000000\n 6 Connecticut       661000000\n 7 New York        10341000000\n 8 New Jersey        730000000\n 9 Pennsylvania     7124000000\n10 Delaware          130000000\n# ℹ 2,390 more rows\n\n\n\n\n\n4.1.1 arrange() - Sorting Data\nThe arrange() function is used to sort data by one or more variables.\n\n# Arrange data by year in descending order\nsorted_data &lt;- state_milk_production %&gt;%\n  arrange(desc(year))\n\nprint(sorted_data)\n\n# A tibble: 2,400 × 4\n   region    state          year milk_produced\n   &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Northeast Maine          2017     630000000\n 2 Northeast New Hampshire  2017     273000000\n 3 Northeast Vermont        2017    2728000000\n 4 Northeast Massachusetts  2017     211000000\n 5 Northeast Rhode Island   2017      13000000\n 6 Northeast Connecticut    2017     420000000\n 7 Northeast New York       2017   14912000000\n 8 Northeast New Jersey     2017     119000000\n 9 Northeast Pennsylvania   2017   10938000000\n10 Northeast Delaware       2017      93000000\n# ℹ 2,390 more rows"
  },
  {
    "objectID": "manip.html#extended-data-manipulation-with-dplyr",
    "href": "manip.html#extended-data-manipulation-with-dplyr",
    "title": "4  Data manipulation",
    "section": "4.2 Extended Data Manipulation with dplyr",
    "text": "4.2 Extended Data Manipulation with dplyr\nIn order to demonstrate advanced data manipulation with the dplyr package, we’ll begin by importing another dataset from the United States Department of Agriculture (USDA), known as the ‘milkcow_facts.csv’ dataset.\nYou can access the original data here. The dataset contains the following variables:\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nyear\ndate\nYear\n\n\navg_milk_cow_number\ndouble\nAverage number of milk cows\n\n\nmilk_per_cow\ndouble\nAverage milk production per cow in pounds\n\n\nmilk_production_lbs\ndouble\nTotal milk production in pounds\n\n\navg_price_milk\ndouble\nAverage price paid for milk (dollars per pound)\n\n\ndairy_ration\ndouble\nAverage price paid for dairy cow rations (dollars per pound)\n\n\nmilk_feed_price_ratio\ndouble\nRatio of average price of milk per dairy cow ration\n\n\nmilk_cow_cost_per_animal\ndouble\nAverage cost of a milk cow per animal (dollars)\n\n\nmilk_volume_to_buy_cow_in_lbs\ndouble\nMilk volume required to purchase a cow (pounds)\n\n\nalfalfa_hay_price\ndouble\nAlfalfa hay price received by farmers (tons)\n\n\nslaughter_cow_price\ndouble\nSlaughter cow price (value of meat in dollars per pound)\n\n\n\nThe subsequent code imports data using the method previously introduced in the preceding chapter:\n\n# Define the URL of the dataset\ndata_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milkcow_facts.csv\"\n\n# Import data from the internet\nmilk_cow_facts &lt;- read_csv(data_url, show_col_types = FALSE)\n\n\njoin() - Combining datasets\nThe join() function is used to combine multiple datasets based on common keys, allowing you to create more complex datasets.\n\n# Joining two datasets based on a common variable (e.g., year)\ncombined_data &lt;- milk_cow_facts |&gt;\n  left_join(grouped_data, by = \"year\")\n\nprint(combined_data)\n\n# A tibble: 35 × 12\n    year avg_milk_cow_number milk_per_cow milk_production_lbs avg_price_milk\n   &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n 1  1980            10799000        11891        128406000000          0.13 \n 2  1981            10898000        12183        132770000000          0.138\n 3  1982            11011000        12306        135505000000          0.136\n 4  1983            11059000        12622        139588000000          0.136\n 5  1984            10793000        12541        135351000000          0.135\n 6  1985            10981000        13024        143012000000          0.127\n 7  1986            10773000        13285        143124000000          0.125\n 8  1987            10327000        13819        142709000000          0.125\n 9  1988            10224000        14185        145034000000          0.122\n10  1989            10046000        14323        143893000000          0.136\n# ℹ 25 more rows\n# ℹ 7 more variables: dairy_ration &lt;dbl&gt;, milk_feed_price_ratio &lt;dbl&gt;,\n#   milk_cow_cost_per_animal &lt;dbl&gt;, milk_volume_to_buy_cow_in_lbs &lt;dbl&gt;,\n#   alfalfa_hay_price &lt;dbl&gt;, slaughter_cow_price &lt;dbl&gt;, avg_milk_produced &lt;dbl&gt;\n\n\n\n\n\nAdvanced Filtering with between()\nIn data analysis, advanced filtering often requires the ability to select rows based on specific numeric ranges. The between() function in R is a powerful tool that enables advanced filtering by allowing you to extract rows where a variable falls within a defined range. Let’s explore how to use this function for more complex filtering tasks. For instance, suppose you want to perform advanced filtering to select rows with years falling within the range of 2015 to 2020:\n\n# Advanced filtering: Select data for a specific range of years\nfiltered_data &lt;- state_milk_production |&gt;\n  filter(between(year, 2015, 2020))\n\nprint(filtered_data)\n\n# A tibble: 150 × 4\n   region    state          year milk_produced\n   &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Northeast Maine          2015     594000000\n 2 Northeast New Hampshire  2015     282000000\n 3 Northeast Vermont        2015    2666000000\n 4 Northeast Massachusetts  2015     217000000\n 5 Northeast Rhode Island   2015      16000000\n 6 Northeast Connecticut    2015     396000000\n 7 Northeast New York       2015   14094000000\n 8 Northeast New Jersey     2015     127000000\n 9 Northeast Pennsylvania   2015   10800000000\n10 Northeast Delaware       2015      99000000\n# ℹ 140 more rows\n\n\n\n\n\ncase_when() - Conditional transformations filtered_data\nThe case_when() function is used to perform conditional transformations, allowing you to create new variables based on multiple conditions.\n\n# Create a new variable \"milk_prod\" based on conditions\ntransformed_data &lt;- milk_cow_facts |&gt;\n  mutate(milk_prod = case_when(\n    milk_per_cow &gt; 19000 ~ \"High\",\n    milk_per_cow &gt; 14000 ~ \"Medium\",\n    TRUE ~ \"Low\"\n  ))\n\nprint(transformed_data)\n\n# A tibble: 35 × 12\n    year avg_milk_cow_number milk_per_cow milk_production_lbs avg_price_milk\n   &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n 1  1980            10799000        11891        128406000000          0.13 \n 2  1981            10898000        12183        132770000000          0.138\n 3  1982            11011000        12306        135505000000          0.136\n 4  1983            11059000        12622        139588000000          0.136\n 5  1984            10793000        12541        135351000000          0.135\n 6  1985            10981000        13024        143012000000          0.127\n 7  1986            10773000        13285        143124000000          0.125\n 8  1987            10327000        13819        142709000000          0.125\n 9  1988            10224000        14185        145034000000          0.122\n10  1989            10046000        14323        143893000000          0.136\n# ℹ 25 more rows\n# ℹ 7 more variables: dairy_ration &lt;dbl&gt;, milk_feed_price_ratio &lt;dbl&gt;,\n#   milk_cow_cost_per_animal &lt;dbl&gt;, milk_volume_to_buy_cow_in_lbs &lt;dbl&gt;,\n#   alfalfa_hay_price &lt;dbl&gt;, slaughter_cow_price &lt;dbl&gt;, milk_prod &lt;chr&gt;\n\n\n\n\n\nmutate_at() - Applying functions to multiple columns.\nThe mutate_at() function allows you to apply a function to multiple columns simultaneously.\n\n# Apply the transformation to selected columns\ntransformed_data &lt;- milk_cow_facts |&gt;\n  mutate_at(vars(milk_production_lbs, milk_per_cow), function(x) x * 0.453592)\n\nprint(transformed_data)\n\n# A tibble: 35 × 11\n    year avg_milk_cow_number milk_per_cow milk_production_lbs avg_price_milk\n   &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n 1  1980            10799000        5394.         58243934352          0.13 \n 2  1981            10898000        5526.         60223409840          0.138\n 3  1982            11011000        5582.         61463983960          0.136\n 4  1983            11059000        5725.         63316000096          0.136\n 5  1984            10793000        5688.         61394130792          0.135\n 6  1985            10981000        5908.         64869099104          0.127\n 7  1986            10773000        6026.         64919901408          0.125\n 8  1987            10327000        6268.         64731660728          0.125\n 9  1988            10224000        6434.         65786262128          0.122\n10  1989            10046000        6497.         65268713656          0.136\n# ℹ 25 more rows\n# ℹ 6 more variables: dairy_ration &lt;dbl&gt;, milk_feed_price_ratio &lt;dbl&gt;,\n#   milk_cow_cost_per_animal &lt;dbl&gt;, milk_volume_to_buy_cow_in_lbs &lt;dbl&gt;,\n#   alfalfa_hay_price &lt;dbl&gt;, slaughter_cow_price &lt;dbl&gt;\n\n\n\n\n\ngroup_by() with multiple variables - Multi-level grouping\nYou can use group_by() with multiple variables to create multi-level grouping for more complex summaries.\n\n# Group data by region and year and calculate summary statistics\nmulti_level_grouped_data &lt;- state_milk_production |&gt;\n  group_by(region, year) |&gt;\n  summarize(total_milk_produced = sum(milk_produced))\n\nprint(multi_level_grouped_data)\n\n# A tibble: 480 × 3\n# Groups:   region [10]\n   region       year total_milk_produced\n   &lt;chr&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1 Appalachian  1970          8202000000\n 2 Appalachian  1971          8216000000\n 3 Appalachian  1972          8389000000\n 4 Appalachian  1973          7900000000\n 5 Appalachian  1974          7858000000\n 6 Appalachian  1975          7953000000\n 7 Appalachian  1976          8292000000\n 8 Appalachian  1977          8434000000\n 9 Appalachian  1978          8109000000\n10 Appalachian  1979          8163000000\n# ℹ 470 more rows"
  },
  {
    "objectID": "manip.html#exercises",
    "href": "manip.html#exercises",
    "title": "4  Data manipulation",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\nExercise 1: Filtering and Selecting\nFilter the milk_cow_facts dataset to retain only rows for the year 2015. Then, select and display the total milk production in pounds (milk_produced).\n\n# Exercise 1: Filtering and Selecting\nfiltered_data &lt;- state_milk_production |&gt;\n  filter(year == 2015) |&gt;\n  select(state, milk_produced)\n\nfiltered_data\n\n# A tibble: 50 × 2\n   state         milk_produced\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Maine             594000000\n 2 New Hampshire     282000000\n 3 Vermont          2666000000\n 4 Massachusetts     217000000\n 5 Rhode Island       16000000\n 6 Connecticut       396000000\n 7 New York        14094000000\n 8 New Jersey        127000000\n 9 Pennsylvania    10800000000\n10 Delaware           99000000\n# ℹ 40 more rows\n\n\n\n\n\nExercise 2: Grouping and Summarizing\nGroup the milkcow_facts dataset by the year variable. Calculate the average milk production per cow (milk_per_cow) for each year.\n\n# Exercise 2: Grouping and Summarizing\nsummary_data &lt;- milk_cow_facts |&gt;\n  group_by(year) |&gt;\n  summarize(avg_milk_per_cow = mean(milk_per_cow, na.rm = TRUE))\n\nsummary_data\n\n# A tibble: 35 × 2\n    year avg_milk_per_cow\n   &lt;dbl&gt;            &lt;dbl&gt;\n 1  1980            11891\n 2  1981            12183\n 3  1982            12306\n 4  1983            12622\n 5  1984            12541\n 6  1985            13024\n 7  1986            13285\n 8  1987            13819\n 9  1988            14185\n10  1989            14323\n# ℹ 25 more rows\n\n\n\n\n\nExercise 3: Calculating New Variables\nIn the milk_cow_facts dataset, calculate the ratio of the average price of milk per dairy cow ration (milk_feed_price_ratio) by dividing the average price paid for milk (avg_price_milk) by the average price paid for dairy cow rations (dairy_ration) in 2013.\n\n# Exercise 3: Calculating New Variables and filtering\nmilkcow_facts_2013 &lt;- milk_cow_facts |&gt;\n  filter(year == 2013) |&gt;\n  mutate(milk_feed_price_ratio = avg_price_milk / dairy_ration)\n\nmilkcow_facts_2013\n\n# A tibble: 1 × 11\n   year avg_milk_cow_number milk_per_cow milk_production_lbs avg_price_milk\n  &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n1  2013             9224000        21816        201231000000          0.201\n# ℹ 6 more variables: dairy_ration &lt;dbl&gt;, milk_feed_price_ratio &lt;dbl&gt;,\n#   milk_cow_cost_per_animal &lt;dbl&gt;, milk_volume_to_buy_cow_in_lbs &lt;dbl&gt;,\n#   alfalfa_hay_price &lt;dbl&gt;, slaughter_cow_price &lt;dbl&gt;\n\n\n\n\n\nExercise 4: Filter and sorting Data\nSort the state_milk_production dataset in descending order based on the milk_produced variable in 2014. Display the top 10 states with the highest production of milk in this year.\n\n# Exercise 4: Sorting Data\nsorted_data &lt;- state_milk_production %&gt;%\n  filter(year == 2014) %&gt;%\n  arrange(desc(milk_produced)) %&gt;%\n  head(10)\n\nsorted_data\n\n# A tibble: 10 × 4\n   region          state         year milk_produced\n   &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 Pacific         California    2014   42339000000\n 2 Lake States     Wisconsin     2014   27795000000\n 3 Mountain        Idaho         2014   13873000000\n 4 Northeast       New York      2014   13730000000\n 5 Northeast       Pennsylvania  2014   10664000000\n 6 Southern Plains Texas         2014   10310000000\n 7 Lake States     Michigan      2014    9609000000\n 8 Lake States     Minnesota     2014    9127000000\n 9 Mountain        New Mexico    2014    8105000000\n10 Pacific         Washington    2014    6576000000\n\n\n\n\n\nExercise 5: Preparing Data for ggplot2 Visualization\nFor our upcoming exercise, we will be working with another two datasets: milk_products_facts and clean_cheese. These datasets offer valuable insights into consumption patterns of various dairy and cheese products in the United States over the years.\nTo import the milk_products_facts dataset, use the following code:\n\n# Define the URL of the dataset for milk products\nmilk_products_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milk_products_facts.csv\"\n\n# Import data from the internet\nmilk_products_facts &lt;- read.csv(milk_products_url)\n\nTo import the clean_cheese dataset, use the following code:\n\n# Define the URL of the dataset for clean cheese products\nclean_cheese_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/clean_cheese.csv\"\n\n# Import data from the internet\nclean_cheese &lt;- read.csv(clean_cheese_url)\n\nHere’s a complete overview of the variables within each dataset:\nmilk_products_facts:\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nyear\ndate\nYear\n\n\nfluid_milk\ndouble\nAverage milk consumption in pounds per person\n\n\nfluid_yogurt\ndouble\nAverage yogurt consumption in pounds per person\n\n\nbutter\ndouble\nAverage butter consumption in pounds per person\n\n\ncheese_american\ndouble\nAverage American cheese consumption in pounds per person\n\n\ncheese_other\ndouble\nAverage other cheese consumption in pounds per person\n\n\ncheese_cottage\ndouble\nAverage cottage cheese consumption in pounds per person\n\n\nevap_cnd_canned_whole_milk\ndouble\nAverage evaporated and canned whole milk consumption in pounds per person\n\n\nevap_cnd_bulk_whole_milk\ndouble\nAverage evaporated and canned bulk whole milk consumption in pounds per person\n\n\nevap_cnd_bulk_and_can_skim_milk\ndouble\nAverage evaporated and canned bulk and canned skim milk consumption in pounds per person\n\n\nfrozen_ice_cream_regular\ndouble\nAverage regular frozen ice cream consumption in pounds per person\n\n\nfrozen_ice_cream_reduced_fat\ndouble\nAverage reduced-fat frozen ice cream consumption in pounds per person\n\n\nfrozen_sherbet\ndouble\nAverage frozen sherbet consumption in pounds per person\n\n\nfrozen_other\ndouble\nAverage consumption of other frozen milk products in pounds per person\n\n\ndry_whole_milk\ndouble\nAverage consumption of dry whole milk in pounds per person\n\n\ndry_nonfat_milk\ndouble\nAverage consumption of dry nonfat milk in pounds per person\n\n\ndry_buttermilk\ndouble\nAverage consumption of dry buttermilk in pounds per person\n\n\ndry_whey\ndouble\nAverage consumption of dry whey (milk protein) in pounds per person\n\n\n\nclean_cheese:\n\n\n\n\n\n\n\n\nVariable\nClass\nDescription\n\n\n\n\nYear\ndate\nYear\n\n\nCheddar\ndouble\nCheddar consumption in pounds per person\n\n\nAmerican Other\ndouble\nAmerican Other consumption in pounds per person\n\n\nMozzarella\ndouble\nMozzarella consumption in pounds per person\n\n\nItalian Other\ndouble\nItalian Other consumption in pounds per person\n\n\nSwiss\ndouble\nSwiss consumption in pounds per person\n\n\nBrick\ndouble\nBrick consumption in pounds per person\n\n\nMuenster\ndouble\nMuenster consumption in pounds per person\n\n\nCream and Neufchatel\ndouble\nCream and Neufchatel consumption in pounds per person\n\n\nBlue\ndouble\nBlue consumption in pounds per person\n\n\nOther Dairy Cheese\ndouble\nOther Dairy Cheese consumption in pounds per person\n\n\nProcessed Cheese\ndouble\nProcessed Cheese consumption in pounds per person\n\n\nFoods and spreads\ndouble\nFoods and spreads consumption in pounds per person\n\n\nTotal American Cheese\ndouble\nTotal American Cheese consumption in pounds per person\n\n\nTotal Italian Cheese\ndouble\nTotal Italian Cheese consumption in pounds per person\n\n\nTotal Natural Cheese\ndouble\nTotal Natural Cheese consumption in pounds per person\n\n\nTotal Processed Cheese Products\ndouble\nTotal Processed Cheese Products consumption in pounds per person\n\n\n\nThese two datasets, like the previous ones, are also sourced from the same GitHub repository metioned before.\nQuestion\nIn this exercise, we will analyze and visualize the trends in cheese consumption in the United States. Specifically, we aim to understand the relationship between the consumption of American cheese and mozzarella cheese over the years.\nInstructions:\n\nSelect relevant variables from each dataset. From “milk_products_facts,” select “year” and “cheese_american.” From “clean_cheese,” select “Year” and “Mozzarella.”\nMerge the datasets using a common variable, which is “year.”\nCalculate the ratio of American cheese consumption to mozzarella consumption for each year.\nCreate a line plot using the ggplot2 package to visualize how the ratio of American cheese consumption to mozzarella consumption has changed over the years.\n\nAnswer:\n\n# Perform data manipulation\nmilk_products_facts &lt;- milk_products_facts %&gt;%\n  select(year, cheese_american) \n\nclean_cheese &lt;- clean_cheese %&gt;%\n  select(Year, Mozzarella) \n\n# Merge the datasets on the \"year\" variable\nmerged_data &lt;- left_join(milk_products_facts, clean_cheese, by = c(\"year\" = \"Year\"))\n\n# Calculate the ratio of American cheese consumption to mozzarella consumption\nmerged_data &lt;- merged_data %&gt;%\n  mutate(ratio = cheese_american / Mozzarella)\n\n# Create a line plot\nggplot(merged_data, aes(x = year, y = ratio)) +\n  geom_point() +\n  geom_line(color = \"blue\") +\n  labs(\n    x = \"Year\",\n    y = \"Ratio (American Cheese / Mozzarella)\",\n    title = \"Ratio of American Cheese to Mozzarella Consumption Over the Years\"\n  )"
  },
  {
    "objectID": "visual.html#ggplot2-grammar",
    "href": "visual.html#ggplot2-grammar",
    "title": "5  Data visualization",
    "section": "5.1 ggplot2 grammar",
    "text": "5.1 ggplot2 grammar\nggplot2, is built on the foundation of the “Grammar of Graphics.” This framework allows you to create intricate and customized plots by layering (Figure 5.1) different geometric shapes and elements. Understanding the concept of geometry and layering is essential for informative visualizations.\n\n\n\nFigure 5.1: Source: Quebec Centre for Biodiversity Science (2022), Chapter 5 in Workshop 3: Introduction to Data Visualization. source.\n\n\nBelow, we present a breakdown of different elements and common parameters for each within the Grammar of Graphics framework:\n\n5.1.1 Basic elements\nData: This refers to your dataset, organized in a tidy format, and serves as the foundation for your plot. You can utilize dplyr techniques to prepare the data for optimal plotting, usually requiring one row for each observation you intend to visualize.\nAesthetics (aes): Aesthetics are parameters that make the data visually meaningful. Here are some common aesthetics:\n\nx, y: Represent variables along the x and y axes, respectively.\ncolour: Dictates the color of the graphic elements based on data.\nfill: Specifies the interior color of the graphical element.\ngroup: Defines the group to which a graphical element belongs.\nshape: Determines the figure used for plotting a point.\nlinetype: Specifies the type of line to be used (e.g., solid, dashed).\nsize: Controls the size of elements to represent an additional dimension.\nalpha: Governs the transparency level of the graphical element.\n\nGeometric Objects (geoms): Geoms determine the type of plot to create. Here are some common geoms:\n\ngeom_point(): Used for scatterplots.\ngeom_line(): Connects points in increasing order of x values.\ngeom_path(): Connects points in the sequence of appearance.\ngeom_boxplot(): Generates box and whisker plots for categorical variables.\ngeom_bar(): Creates bar charts, typically used for categorical x-axes.\ngeom_histogram(): Produces histograms, commonly for continuous x-axes.\ngeom_violin(): Represents the kernel of data dispersion in a distribution.\ngeom_smooth(): Generates a function line based on data.\n\nFacets: Faceting helps create small multiples, allowing you to visualize subsets of your data. You can use facet_wrap() or facet_grid() for this purpose.\nStatistics: Statistics are similar to geoms, but they are computed values that summarize your data. They can show means, counts, and other statistical summaries.\nCoordinates: Coordinates determine how data is mapped onto the plot. Here are some coordinate options:\n\ncoord_cartesian: Sets limits for the plot.\ncoord_polar: Used for circular plots.\ncoord_map: Applied for various map projections.\n\nThemes: Themes define the overall visual defaults for your plot. You can use themes to specify fonts, colors, shapes, and outlines to achieve a consistent visual style.\nNext, we will demonstrate the layering grammar of ggplot2. On this example we will learn how to create a scatterplot with ggplot2 and add layers, geometries, and configurations step by step using simulated dairy cattle production data.\nStep 1: Inserting data and Aesthetics\n\n# Simulated dairy cattle production data\nset.seed(123)\ncattle_data &lt;- data.frame(\n  Year = 2000:2020,\n  Milk_Production = runif(21, min = 8000, max = 12000)) |&gt;\n  mutate(Cheese_Production = (Milk_Production * 0.3) + rnorm(21, 0, 100))\n\n# Create a ggplot object with data and aesthetics\nscatter_plot &lt;- ggplot(cattle_data, aes(x = Milk_Production, y = Cheese_Production))\nscatter_plot\n\n\n\n\nStep 2: Adding the Scatterplot (points) Geometry\n\n# Add the scatterplot geometry using geom_point\nscatter_plot &lt;- scatter_plot + geom_point()\nscatter_plot\n\n\n\n\nStep 3: Adding Labels and Title\n\n# Add labels and a title\nscatter_plot &lt;- scatter_plot +\n  labs(\n    x = \"Milk Production (Millions of Pounds)\",\n    y = \"Cheese Production (Millions of Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production\"\n  )\nscatter_plot\n\n\n\n\nStep 4: Applying a Minimal Theme\n\n# Apply a minimal theme\nscatter_plot &lt;- scatter_plot + theme_minimal()\n\n# Display the scatter plot\nprint(scatter_plot)"
  },
  {
    "objectID": "visual.html#exploring-geometries",
    "href": "visual.html#exploring-geometries",
    "title": "5  Data visualization",
    "section": "5.2 Exploring Geometries",
    "text": "5.2 Exploring Geometries\nIn this instance, we will present different geometric shapes and their application in ggplot2 visualizations.\n\ngeom_line() for simple Line Plot\nIn ggplot2, a line plot is often utilized in conjunction with a scatterplot, as they are frequently applied to the same dataset and serve the same visualization purpose. It is not uncommon for both types of plots to be used together.\n\n# Create a ggplot object for a scatterplot and line plot\nline_plot &lt;- ggplot(cattle_data, aes(x = Milk_Production, y = Cheese_Production)) +\n  geom_point() +  # Scatterplot (points)\n  geom_line() +   # Line plot\n  labs(\n    x = \"Milk Production (Millions of Pounds)\",\n    y = \"Cheese Production (Millions of Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production\"\n  ) +\n  theme_minimal()\nline_plot  # Display the combined plot\n\n\n\n\nThis type of plot is commonly employed to visualize and describe time series data, particularly in the context of climate and environmental data over time.\n\n# Create a sample time series dataset\nset.seed(123)\ndate_range &lt;- seq(as.Date(\"2023-01-01\"), as.Date(\"2023-12-01\"), by=\"1 month\")\ntemperature_data &lt;- data.frame(\n  Date = date_range,\n  Temperature = runif(length(date_range), min = 0, max = 30)\n)\n\n# Create a time series plot\nggplot(temperature_data, aes(x = Date, y = Temperature)) +\n  geom_point() + \n  geom_line() +\n  labs(\n    x = \"Date\",\n    y = \"Temperature (°C)\",\n    title = \"Monthly Temperature Time Series\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\ngeom_histogram() for Histograms\nHistograms are a commonly employed data visualization tool to illustrate the frequency distribution of a specific variable. To illustrate this, we will use the “state_milk_production” dataset, filtering it for a particular year (as it is not meaningful to examine the distribution of a variable measured over multiple years).\n\n# Filter the dataset for a specific year and select the 'milk_produced' column\nyear_to_analyze &lt;- 1980\nfiltered_data &lt;- state_milk_production |&gt;\n  filter(year == year_to_analyze) |&gt;\n  select(milk_produced)\n\n# Create a histogram\nggplot(filtered_data, aes(x = milk_produced)) +\n  geom_histogram(bins = 8, boundary=0) +\n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Frequency\",\n    title = paste(\"Milk Production Distribution for the Year\", year_to_analyze)\n  ) +\n  theme_minimal()\n\n\n\n\n\n\ngeom_bar() for barplots\n\n# Filter data to focus on the top 5 states with the highest milk production\ntop_states &lt;- state_milk_production |&gt;\n  group_by(state) |&gt;\n  summarise(avg_Milk = mean(milk_produced)) |&gt;\n  arrange(desc(avg_Milk)) |&gt;\n  head(5)\n\n# Create a bar plot to compare milk production in the top 5 states\nbar_plot_top_states &lt;- ggplot(top_states, aes(x = reorder(state, -avg_Milk), y = avg_Milk)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    x = \"State\",\n    y = \"Average Milk Production (Pounds)\",\n    title = \"Top 5 States with Highest Milk Production\"\n  ) +\n  theme_minimal()\n\n# Display the bar plot\nprint(bar_plot_top_states)\n\n\n\n\n\n\ngeom_smooth() for scatter plots with linear fit\nExpanding upon a dataset that includes data on milk and cheese production in various states and regions, we move beyond the confines of a basic linear plot. By introducing trendlines, we empower ourselves to delve deeper into the intricacies of the relationships between these variables.\n\n# Create a linear plot with trendlines\nlinear_plot &lt;- ggplot(cattle_data, aes(x = Milk_Production, y = Cheese_Production)) +\n  geom_point() +  # Scatterplot points\n  geom_smooth(method = \"lm\", formula = y ~ x) +  # Linear trendlines\n  labs(\n    x = \"Milk Production (Millions of Pounds)\",\n    y = \"Cheese Production (Millions of Pounds)\",\n    title = \"Linear Relationship Between Milk and Cheese Production\") +\n  theme_minimal()\n\nlinear_plot\n\n\n\n\n\n\ngeom_boxplot() and geom_violin() for Boxplots and Violin plots\nSimilar to histograms, boxplots serve the purpose of visualizing data distributions. However, they are commonly employed when the goal is to compare these distributions among different groups.\n\n# Filter the data to include only New York and California\ndf_groups &lt;- state_milk_production |&gt;\n  filter(state %in% c(\"New York\", \"California\"))\n\n# Boxplot\nggplot(data = df_groups, \n       aes(x = state, \n           y = milk_produced)) + \n  geom_boxplot() +  # Create a boxplot\n  labs(title = \"Boxplot\") +  # Add a title\n  theme_minimal()  # Apply a minimal theme\n\n\n\n# Violin plot\nggplot(data = df_groups, \n       aes(x = state, \n           y = milk_produced)) + \n  geom_violin() +  # Create a violin plot\n  labs(title = \"Violin Plot\") +  # Add a title\n  theme_minimal()  # Apply a minimal theme\n\n\n\n\n\n\ngeom_maps() for representing maps\nMaps play a fundamental role in data visualization by providing a spatial context that aids in revealing patterns, relationships, and trends in various data sets, such as demographics, environmental factors, and resource distribution.\nIn this context, we will explore the spatial distribution of milk production across the United States.\n\n# Create a new dataframe 'df' based on the 'state_milk_production' dataset\ndf &lt;- state_milk_production |&gt; \n  # Group the data by 'state'\n  group_by(state) |&gt;\n  # Calculate the average milk production for each state\n  summarise(avg_milk = mean(milk_produced))\n\n\n# load maps library\nlibrary(maps, mapproj)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nstates_map &lt;- map_data(\"state\") # get a state-level map of the US\n\n# Let's make a new column for the states name\n# that matches the column of state names in our states_map\ndf$region &lt;- tolower(df$state)\n\n# Build our map\nggplot(df, \n       aes(map_id = region)) + # the variable name to link our map and dataframe\n  geom_map(aes(fill = avg_milk), # variable we want to represent with an aesthetic\n           map = states_map) + # data frame that contains coordinates\n  expand_limits(x = states_map$long, \n                y = states_map$lat) +\n  coord_map() +  # projection \n  labs(x = \"\", y = \"\") # remove axis labels"
  },
  {
    "objectID": "visual.html#faceting-for-complex-data",
    "href": "visual.html#faceting-for-complex-data",
    "title": "5  Data visualization",
    "section": "5.3 Faceting for Complex Data",
    "text": "5.3 Faceting for Complex Data\nFacetting in ggplot2 is a powerful technique that allows you to create multiple small plots, or facets, within a single visualization. Each facet typically represents a subset of your data, making it an effective way to compare and visualize multiple aspects of your dataset simultaneously. In this example, we will explore faceting in ggplot2 and demonstrate its utility through two different scenarios.\n\nExample 1:\nIn the first example, we’ll use faceting to create a bar plot that shows the average milk production in the states of New York and California over the years 2010 to 2014. This will help us visualize how milk production varies in these two states during this time period.\n\ndf_2 &lt;- state_milk_production |&gt;\n  filter(state %in% c(\"New York\", \"California\"), between(year, 2010, 2014)) |&gt;\n  group_by(state, year) |&gt;\n  summarise(avg_milk = mean(milk_produced))\n\n`summarise()` has grouped output by 'state'. You can override using the\n`.groups` argument.\n\n# Create a faceted bar plot for milk production by state\nggplot(df_2, aes(x = state, y = avg_milk)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  facet_wrap(~year, nrow = 1) +  # Facet by year with one row\n  labs(\n    x = \"State\",\n    y = \"Milk Production (Pounds)\",\n    title = \"Faceted Bar Plot of Milk Production by Year\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nExample 2:\nIn the second example, we’ll use facetting to create a scatterplot to compare the relationship between milk and cheese production for different regions. This will allow us to visually explore how these two variables are related in two different states.\n\n# Set the random seed for reproducibility\nset.seed(123)\n\n# Create a dataframe for cattle data with two states\ncattle_data_2 &lt;- data.frame(\n  Year = 2000:2021,\n  State = rep(c(\"California\", \"New York\"), each = 11),  # Assign two states alternately\n  Milk_Production = runif(22, min = 8000, max = 12000)  # Simulate milk production\n) |&gt;\nmutate(\n  Cheese_Production = case_when(\n    State == \"California\" ~ (Milk_Production * 0.3) + rnorm(22, 0, 100),\n    State == \"New York\" ~ (Milk_Production * 0.25) + rnorm(22, 0, 80)\n  )\n)\n\n# Create a faceted scatterplot with cattle data for milk vs. cheese production by region\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production)) +\n  geom_point(color = \"blue\") +\n  facet_wrap(~State, ncol = 2) +  # Facet by region with 2 columns\n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Faceted Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "visual.html#configuring-aesthetics",
    "href": "visual.html#configuring-aesthetics",
    "title": "5  Data visualization",
    "section": "5.4 Configuring aesthetics",
    "text": "5.4 Configuring aesthetics\nNow, let’s explore the fundamental aspects of the aesthetic layer in data visualization. Aesthetics, as specified with aes() in ggplot2, enable us to distinguish between classes, groups, and data structures effectively.\nIn this section, we will examine key aesthetic properties, such as color, shape, size, labels, and transparency. These attributes function as essential tools for tailoring your visualizations to specific requirements, allowing you to craft informative, customizable plots that faithfully represent your data’s insights.\n\n\n5.4.1 Colour\n1. Groups (discrete variables)\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal()\n\n\n\n\n2. Gradients (continous variables)\n\nggplot(cattle_data_2, aes(x = Year, y = Cheese_Production, col = Milk_Production)) +\n  geom_point() +\n  labs(\n    x = \"Year\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal()\n\n\n\n\n2. Manual colors\nDiscrete variables\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal() +\n  scale_colour_manual(values = c(\"orange\",\"skyblue\"))\n\n\n\n\nGradient\n\nggplot(cattle_data_2, aes(x = Year, y = Cheese_Production, col = Milk_Production)) +\n  geom_point() +\n  labs(\n    x = \"Year\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal() +\n  scale_color_gradient(low = \"gray85\", high = \"black\")\n\n\n\n\n\n\n\n5.4.2 Shape Size and alpha\nLet’s move past colors for a moment. If we want to change how our data points look, we can use different shapes for different groups by specifying the “shape” parameter within the “aes()” layer. This is similar to what we did with colors. For instance, we can use the “species” variable to group our data points by shape.\nIf we’d like to control the size of the data points, we can use the “size” parameter in the same way. And if we want to make data points more or less transparent, we can use the “alpha” parameter. Both of these options work well when dealing with continuous data.\nShape for discrete variables\n\nggplot(cattle_data_2) +\n  geom_point(aes(x = Milk_Production, y = Cheese_Production, shape = State)) +  \n  geom_smooth(method = \"lm\", formula = y~x, aes(x = Milk_Production, y = Cheese_Production, linetype = State)) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal() \n\n\n\n\nSize and Alpha for continous\n\nggplot(cattle_data_2) +\n  geom_point(aes(x = Year, y = Cheese_Production, size = Milk_Production)) +\n  labs(\n    x = \"Year\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal() \n\n\n\n\nalpha\n\nggplot(cattle_data_2) +\n  geom_point(aes(x = Year, y = Cheese_Production, alpha = Milk_Production)) +\n  labs(\n    x = \"Year\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n5.4.3 Legend and other stuffs\nNext, we’ll explore how to customize legends and make other adjustments. Specifically, we will focus on modifying legends and adding error bars and labels above barplots in ggplot2.\n\nModifying Legends:\n\nBy default, ggplot2 automatically generates legends for aesthetic mappings in your plot. You can customize legends in various ways:\n\nChange Legend Title: You can set the legend title using the labs function and specifying fill (for color) or shape as appropriate.\n\n\n+ labs(fill = \"Custom Legend Title\")\n\n\nRemove Legend: If you want to remove a legend for a specific aesthetic mapping, use guides() with override.aes.\n\n\n+ guides(fill = FALSE)\n\n\nChange Legend Labels: You can change legend labels using scale_fill_manual() or scale_shape_manual() functions.\n\n\n+ scale_color_manual(labels = c(\"A\", \"B\"))\n\nLets see an example on how to configure legends:\n\nreplace = c(\"A\" = \"California state\", \"B\" = \"New York State\")\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by states\",\n    col = \"\"\n  ) +\n  theme_minimal() +\n  scale_colour_manual(values = c(\"orange\",\"skyblue\"), labels = c(\"California state\" ,\"New York state\")) \n\n\n\n\n\nAdding Error Bars:\n\nTo add error bars to a barplot, you can use the geom_errorbar() function. For example:\n\n+ geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.4)\n\nHere, lower and upper are the variables that contain the lower and upper error values for each bar.\n\nAdding Letters Above Barplots:\n\nTo add letters above barplots, you can use the geom_text() function. Make sure you have a variable that represents the letters you want to display. Here’s an example:\n\n+ geom_text(aes(label = letters), vjust = -0.5, size = 4)\n\nIn this example, letters is the variable that contains the letters to display, vjust adjusts the vertical position of the text above the bars, and size sets the text size.\nHere’s an example of how to include error bar to barplots:\n\n# Filter data to focus on the top 5 states with the highest milk production\ntop_states &lt;- state_milk_production |&gt;\n  group_by(state) |&gt;\n  summarise(avg_Milk = mean(milk_produced),\n            sd_Milk = sd(milk_produced)) |&gt;\n  mutate(lower = avg_Milk - sd_Milk, upper = avg_Milk + sd_Milk) |&gt;\n  arrange(desc(avg_Milk)) |&gt;\n  head(5)\n\n# Create a bar plot to compare milk production in the top 5 states\np &lt;- ggplot(top_states, aes(x = reorder(state, -avg_Milk), y = avg_Milk)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(\n    x = \"State\",\n    y = \"Average Milk Production (Pounds)\",\n    title = \"Top 5 States with Highest Milk Production\"\n  ) +\n  theme_minimal() +\n  # Add error bars\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.4) +\n  # Add letters above bars\n  geom_text(aes(label = c(\"a\", \"a\", \"b\", \"b\", \"b\"), y = 0), vjust = -0.5, size = 4, col = \"white\")\n\nprint(p)"
  },
  {
    "objectID": "visual.html#configuring-themes",
    "href": "visual.html#configuring-themes",
    "title": "5  Data visualization",
    "section": "5.5 Configuring themes",
    "text": "5.5 Configuring themes\nThroughout this turorial, the plots have consistently utilized the “theme_classic()” setting. This choice was made because the default gray background can sometimes make certain colors less distinct and harder to discern.\nThe ggplot2 package boasts an extensive array of theme elements, far too numerous to cover comprehensively here.\nRather than exploring the multitude of attributes contained within the “theme()” function Figure 5.2, you can initiate your theming journey with theme functions. These theme functions come equipped with a predefined set of elements to begin with. Below, you’ll find some illustrative examples:\n\n\n\nFigure 5.2: ggplot2 theme reference sheet by Isabella Benabaye source.\n\n\n\n5.5.1 Default theme\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  )# +\n\n\n\n  #theme_gray()\n\n\n\n5.5.2 Classic theme\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n5.5.3 Theme minimal\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n5.5.4 Theme dark\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) +\n  theme_dark()\n\n\n\n\nA compelling choice is to establish a consistent theme that spans your entire analysis. For this, we can use the set_theme() option:\n\ntheme_set(theme_minimal()) \n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  ) \n\n\n\n\nUsing the properties detalied in Figure 5.2 you can also set your own theme:\n\nmytheme &lt;- theme_bw() + \n           theme(plot.title = element_text(colour = \"blue\")) +\n           theme(legend.position = \"left\")\n\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  )  + mytheme \n\n\n\n\nWhile ggplot2 offers a set of default themes that are functional and clean, you can expand your design possibilities by integrating the “ggthemes” library. This additional library introduces an array of new themes that cater to specific needs and preferences, from classic and minimalistic to vibrant and distinctive.\n\n#install.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n\n## excel theme\nggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  )  + theme_excel()\n\n\n\n\n\n## tufte theme\np_tufte = ggplot(cattle_data_2, aes(x = Milk_Production, y = Cheese_Production, col = State)) +\n  geom_point() +  \n  geom_smooth(method = \"lm\", formula = y~x) + \n  labs(\n    x = \"Milk Production (Pounds)\",\n    y = \"Cheese Production (Pounds)\",\n    title = \"Scatterplot of Milk vs. Cheese Production by Region\"\n  )  + theme_fivethirtyeight()\n\np_tufte"
  },
  {
    "objectID": "visual.html#arrange-plots",
    "href": "visual.html#arrange-plots",
    "title": "5  Data visualization",
    "section": "5.6 Arrange plots",
    "text": "5.6 Arrange plots\nSometimes is useful to combine one or more plots in one image. R provide several options to do this.\n1. Using grid.arrange()\ngrid.arrange() is a part of the gridExtra package and allows you to arrange multiple ggplot2 plots into a grid layout.\nFirst, make sure you have the gridExtra package installed. You can install it with install.packages(\"gridExtra\").\nHere’s how to use grid.arrange() with examples:\n\nlibrary(gridExtra)\n\n# Create some sample plots\nplot1 &lt;- ggplot(mtcars, aes(x = mpg, y = disp)) + geom_point()\nplot2 &lt;- ggplot(mtcars, aes(x = hp, y = wt)) + geom_point()\n\n# Arrange the plots into a 2x1 grid\ngrid.arrange(plot1, plot2, ncol = 2)\n\n\n\n\n2. Using patchwork\npatchwork is a popular package for arranging ggplot2 plots. It allows you to create complex layouts by combining and arranging plots.\nYou can install the patchwork package with install.packages(\"patchwork\").\nHere’s how to use patchwork with examples:\n\nlibrary(patchwork)\n# Combine the plots horizontally using the `+` operator\ncombined_plots &lt;- plot1 + plot2\ncombined_plots\n\n\n\n\n3. Using | for Simpler Vertical Arrangement\nYou can also arrange ggplot2 plots vertically using the | operator, making it simple and effective.\nHere’s how to use | with examples:\n\n# Arrange the plots vertically using the | operator\ncombined_plots &lt;- plot1 | plot2\ncombined_plots + plot_annotation(tag_levels = \"a\")\n\n\n\n\nIn this tutorial, we’ve covered three methods to arrange ggplot2 plots. You can choose the one that best suits your needs and preferences. These techniques will help you effectively display and compare multiple plots in your data visualization projects."
  },
  {
    "objectID": "visual.html#bonus",
    "href": "visual.html#bonus",
    "title": "5  Data visualization",
    "section": "5.7 Bonus",
    "text": "5.7 Bonus\nPie Chart\n\n# Create a data frame for dairy cattle production\ndata &lt;- data.frame(\n  Activity = c(\"Milk Production\", \"Breeding\", \"Feeding\", \"Health Care\", \"Other\"),\n  Percentage = c(50, 15, 20, 10, 5)\n)\n\n# Create a pie chart\np &lt;- ggplot(data, aes(x = \"\", y = Percentage, fill = Activity)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(theta = \"y\") +\n  scale_fill_manual(values = c(\n    \"Milk Production\" = \"blue\",\n    \"Breeding\" = \"green\",\n    \"Feeding\" = \"red\",\n    \"Health Care\" = \"purple\",\n    \"Other\" = \"gray\"\n  )) +\n  labs(title = \"Dairy Cattle Production Activities\")\n\nprint(p)"
  },
  {
    "objectID": "visual.html#saving-plots",
    "href": "visual.html#saving-plots",
    "title": "5  Data visualization",
    "section": "5.8 Saving plots",
    "text": "5.8 Saving plots\nTo save a plot, we can employ the ggsave function. Within this function, you specify the desired filename, including the file format extension (e.g., “plot.png”). Next, you provide the name of the plot object in R, which, in this case, is “plot.png.” Additionally, you can specify the desired height and width of the saved plot, along with the units of measurement, such as inches.\n\nggsave(filename = \"plot.png\", # Name the file\n       plot = p_tufte, # Name of the plot object in R\n       height = 8.5, # Provide the desired dimensions\n       width = 11, \n       units = \"in\")"
  },
  {
    "objectID": "linear.html#some-theory",
    "href": "linear.html#some-theory",
    "title": "6  Basic linear model",
    "section": "6.1 Some theory",
    "text": "6.1 Some theory\nThe basic linear model is a fundamental tool in statistics used to analyze the relationship between one or more independent variables and a dependent variable. Several assumptions underlie the linear model, including an additive linear relationship, absence of bias, and a Gaussian distribution for error terms. In its simplest form, the linear model is expressed as:\n\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\\]\nwhere:\n\n\\(Y\\) is the dependent variable,\n\\(\\beta_0\\) is the intercept term,\n\\(\\beta_1, \\beta_2, \\dots, \\beta_n\\) are the coefficients for the independent variables \\(X_1, X_2, ..., X_n\\) respectively,\n\\(\\epsilon\\) is the error term.\n\nAssumptions of the Linear Model:\n\nAdditive Linear Relationship:\n\nThe relationship between the independent variables and the dependent variable is assumed to be linear. This means that the change in the mean of the dependent variable is constant for a one-unit change in each independent variable, holding other variables constant.\n\nAbsence of Bias (No endogeneity):\n\nThe model assumes that there is no systematic bias in the relationship between the independent and dependent variables. In other words, the model should not suffer from endogeneity, where omitted variables or measurement errors lead to biased parameter estimates.\n\nGaussian Distribution for Error Terms:\n\nThe error terms (\\(\\epsilon\\)) are assumed to be normally distributed. This assumption is essential for making statistical inferences, such as hypothesis testing and constructing confidence intervals. Deviations from normality might affect the validity of statistical tests.\nThe assumptions of the linear model are crucial for the validity of the statistical inferences drawn from the model. Researchers should assess these assumptions to ensure the reliability of the model’s results in practical applications."
  },
  {
    "objectID": "linear.html#example-in-r-cows-milk-production-experiment",
    "href": "linear.html#example-in-r-cows-milk-production-experiment",
    "title": "6  Basic linear model",
    "section": "6.2 Example in R: Cows’ Milk Production Experiment",
    "text": "6.2 Example in R: Cows’ Milk Production Experiment\nLet’s consider an experiment investigating the impact of three ventilation programs (Vent1, Vent2, Vent3 and Control) on the milk production of cows in a compost barn system. The milk production (in liters) is the dependent variable, and the ventilation programs are the independent variables.\n\n# Load necessary libraries\n#install.packages(\"agricolae\")\nlibrary(tidyverse)\nlibrary(agricolae)\n\nWarning: package 'agricolae' was built under R version 4.3.2\n\n# Create a simulated dataset\nset.seed(123)\nventilation_data &lt;- data.frame(\n  Ventilation = rep(c(\"Vent1\", \"Vent2\", \"Vent3\", \"Control\"), each = 25),\n  MilkProduction = c(rnorm(25, mean = 50, sd = 10),\n                     rnorm(25, mean = 55, sd = 10),\n                     rnorm(25, mean = 60, sd = 10),\n                     rnorm(25, mean = 45, sd = 10))\n)\n\n# Visualize the data\nggplot(ventilation_data, aes(x = Ventilation, y = MilkProduction)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Milk Production by Ventilation Program\",\n       x = \"Ventilation Program\",\n       y = \"Milk Production (liters)\")\n\n\n\n\nThis code generates a simulated dataset representing the milk production of cows under different ventilation programs. The boxplot visualization provides an initial understanding of the data distribution.\nNext, we can fit a basic linear model to quantify the relationship between ventilation programs and milk production:\n\\[Y_{ij} = \\beta_0 + \\beta_jX_{j} + \\epsilon_{ij}\\]\nwhere:\n\n\\(Y_{ij}\\) is the milk production associated to the each animal \\(i\\) and treatment \\(j\\),\n\\(\\beta_0\\) is the intercept term or the overall mean,\n\\(\\beta_j\\) is the coefficient for the independent variable \\(X_j\\) with \\(j\\) representing the four levels of ventilation (\\(j = 1, 2, 3, 4\\)),\n\\(\\epsilon_{ij}\\) is the error term.\n\n\n# Fit a basic linear model\nlm_model &lt;- lm(MilkProduction ~ Ventilation, data = ventilation_data)\n\n# Perform Tukey's HSD test using agricolae\ntukey_results &lt;- LSD.test(lm_model, \"Ventilation\", alpha = 0.05)\n\ntukey_results$means\n\n        MilkProduction      std  r       se      LCL      UCL      Min      Max\nControl       47.82576 8.304201 25 1.837344 44.17866 51.47285 32.79282 66.87333\nVent1         49.66670 9.467324 25 1.837344 46.01960 53.31380 30.33383 67.86913\nVent2         56.02137 9.188734 25 1.837344 52.37428 59.66847 38.13307 76.68956\nVent3         60.10241 9.724214 25 1.837344 56.45531 63.74951 36.90831 80.50085\n             Q25      Q50      Q75\nControl 42.15227 47.38732 54.93504\nVent1   43.74961 47.82025 54.60916\nVent2   50.97115 54.38088 63.21581\nVent3   54.97677 60.53004 64.48210\n\ntukey_results$groups\n\n        MilkProduction groups\nVent3         60.10241      a\nVent2         56.02137      a\nVent1         49.66670      b\nControl       47.82576      b\n\n\nLets provide some graphical visualization of the result:\n\n# Extract the mean differences and group names\ntukey_df &lt;- as.data.frame(tukey_results$means)\ntukey_df$trt &lt;- rownames(tukey_df)\n\n\n# Visualization using ggplot2\ng &lt;- ggplot(tukey_df, aes(x = trt, y = MilkProduction)) +\n  geom_point(size = 3) +\n  geom_errorbar(\n    aes(ymin = LCL, ymax = UCL),\n    width = 0.2) +\n  geom_text(aes(label = as.character(round(MilkProduction, 2))), \n            vjust = -0.5, hjust = 1.1) + \n  labs(title = \"Tukey's test: Milk Production by Ventilation Program\",\n    x = \"Ventilation Program\",\n    y = \"Average milk production\") +\n  theme_minimal()\n\ng + geom_text(data = tukey_df,\n              aes(x = trt, y = UCL + 1, label = c(\"a\", \"a\", \"b\", \"b\")),\n              size = 4, color = \"red\")\n\n\n\n\nAs pointed out earlier, testing the assumptions of a linear model is a crucial step to ensure the reliability of the model’s results. One of the key assumptions is the normality of error terms. Let’s conduct a normality test for the residuals of the linear model using the Shapiro-Wilk test. If the p-value from the test is greater than a significance level (commonly 0.05), we fail to reject the null hypothesis, suggesting that the residuals follow a normal distribution.\nHere’s how you can perform the normality test in R:\n\n# Extract residuals of the linear model\nresiduals &lt;- residuals(lm_model)\n\n# Shapiro-Wilk test for normality\nshapiro_test &lt;- shapiro.test(residuals)\n\n# Visualization of residuals\nqqnorm(residuals)\nqqline(residuals)\n\n\n\n# Display the normality test results\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.99244, p-value = 0.852\n\n\nCorrection:\nBreakdown of the code:\n\nFirst, we extract the residuals using the residuals function.\nThe Shapiro-Wilk test (one of the common normality tests) is performed using shapiro.test on the residuals.\nA Q-Q plot is generated using qqnorm and qqline to visually assess the normality of the residuals.\nThe results of the normality test are printed using print(shapiro_test).\n\nInterpretation: Inspect the p-value from the Shapiro-Wilk test. If the p-value is greater than 0.05, it suggests that we do not have sufficient evidence to reject the null hypothesis of normality. Additionally, examine the Q-Q plot; if the residuals closely follow the reference line, it indicates normality."
  },
  {
    "objectID": "glm_intro.html",
    "href": "glm_intro.html",
    "title": "7  Beyond the Gaussian linear models",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "glm_intro.html#generalized-linear-models-glms",
    "href": "glm_intro.html#generalized-linear-models-glms",
    "title": "7  Beyond the Gaussian linear models",
    "section": "7.1 Generalized Linear Models (GLMs):",
    "text": "7.1 Generalized Linear Models (GLMs):\nGeneralized Linear Models (GLMs) extend the basic linear model, introducing additional flexibility through two key components:\n\nLink Functions: Unlike the linear model, GLMs incorporate link functions to capture the relationship between the linear predictor and the expected value of the response variable. This is particularly beneficial when dealing with non-continuous outcomes, allowing for a more versatile modeling approach (McCullagh and Nelder 1989; Dobson 2002).\nProbability Distributions: GLMs are designed to accommodate a range of probability distributions for the response variable, making them suitable for handling non-Gaussian data. For example, the binomial distribution is frequently utilized in GLMs for binary outcomes (McCullagh and Nelder 1989; Dobson 2002).\n\n\n7.1.1 Example: Binomial Model for Animal Behavior\nLet’s consider a scenario where we’re investigating the occurrence of a certain animal behavior, such as standing estrus in cows. This behavior can be represented as a dichotomous variable (0 for absence, 1 for occurrence). We’ll construct a binomial GLM to model this binary outcome based on different ventilation programs.\nWe will explore the logit linking function, among others like probit and cloglog. The logit function is mathematically defined as:\n\\[g(\\mu) = \\frac{1}{1 + e^{-\\mu}}\\]\nwhere:\n\n\\(g(\\mu)\\) is the probability of success (occurrence),\n\\(\\mu\\) is the linear predictor.\n\nThe linear predictor, \\(\\mu\\), is defined as a linear combination of the predictors:\n\\[\\mu = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_kX_k \\]\nThis mirrors the structure of the linear model discussed in the preceding chapter.\nIn the context of the binomial GLM, the probability of success (\\(p\\)) is related to the linear predictor through the logistic function:\n\\[p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_kX_k)}}\\] Please note that due to the presence of the linking function, it becomes possible to accommodate non-linear relationships between the dependent and independent variables.\n\n\nDevelop the example in R\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(stats)\n\n# Create a sample dataset\nset.seed(123)\nanimal_data &lt;- data.frame(\n  Ventilation = rep(c(\"Vent1\", \n                      \"Vent2\", \n                      \"Vent3\", \n                      \"Control\"),\n                    each = 25),\n  Behavior = c(rbinom(n = 25, size = 1, prob = .6),\n               rbinom(n = 25, size = 1, prob = .4),\n               rbinom(n = 25, size = 1, prob = .4),\n               rbinom(n = 25, size = 1, prob = .7))\n)\n\n# Visualize the data\nggplot(animal_data, aes(x = Ventilation, fill = factor(Behavior))) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Occurrence of 'standing estrus' by Ventilation Program\",\n       x = \"Ventilation Program\",\n       y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n\n\n\n\n\n# Fit a binomial GLM with logit link function\nbinomial_model_logit &lt;- glm(Behavior ~ Ventilation, data = animal_data, family = binomial(link = \"logit\"))\n\n# Display the summary of the model with logit link function\nsummary(binomial_model_logit)\n\n\nCall:\nglm(formula = Behavior ~ Ventilation, family = binomial(link = \"logit\"), \n    data = animal_data)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.3863     0.5000   2.773  0.00556 ** \nVentilationVent1  -1.4663     0.6405  -2.289  0.02206 *  \nVentilationVent2  -2.3308     0.6696  -3.481  0.00050 ***\nVentilationVent3  -1.6275     0.6421  -2.534  0.01126 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.63  on 99  degrees of freedom\nResidual deviance: 123.58  on 96  degrees of freedom\nAIC: 131.58\n\nNumber of Fisher Scoring iterations: 4\n\n\nTo interpret the coefficients of a binomial GLM with a logit link function, you can exponentiate them to obtain odds ratios. The interpretation of odds ratios involves understanding the change in odds for a one-unit increase in the predictor. Here’s how you can create a table and explain the log-odds:\n\n# Create a table for coefficient interpretation\ncoef_table &lt;- exp(coef(binomial_model_logit))\n\n# Display the table\nprint(coef_table)\n\n     (Intercept) VentilationVent1 VentilationVent2 VentilationVent3 \n      4.00000000       0.23076923       0.09722222       0.19642857 \n\n\n\n\n\n\n\n(Intercept)\nVentilationVent1\nVentilationVent2\nVentilationVent3\n\n\n\n\n4\n0.2307692\n0.0972222\n0.1964286\n\n\n\n\n\nThe values in the table represent the odds ratios for each predictor. An odds ratio greater than 1 implies an increase in the odds of success (behavior occurrence), while an odds ratio less than 1 implies a decrease in the odds.\nNow, let’s explain what log-odds means:\nThe log-odds, or logit, is the natural logarithm of the odds. In the context of a logistic regression model, the log-odds can be interpreted as the logarithm of the odds of an event occurring. The transformation from the linear predictor (\\(\\mu\\)) to the log-odds is achieved by the logit function:\n\\[\\text{logit}(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)\\]\nHere, \\(\\mu\\) is the predicted probability of the event (e.g., behavior occurrence). The log-odds represent a linear relationship with the predictors, making it suitable for regression modeling. Exponentiating the coefficients of a logistic regression model yields odds ratios, providing a more interpretable understanding of the impact of predictors on the odds of the event. Let’s interpret each coefficient in the context of the odds ratio:\n\n(Intercept): Represents the odds of the event (e.g., behavior occurrence) in the absence of ventilation. In this scenario, it signifies the odds of occurrence when Ventilation is at the reference level (e.g., Control) or, more precisely, when there is no ventilation.\nVentilationVent1: Captures the effect of Ventilation = “Vent1” relative to the absence of ventilation. An odds ratio of 0.23 suggests that, while holding other variables constant, the odds of the event are 77% lower for Ventilation = “Vent1” compared to the absence of ventilation.\nVentilationVent2: Similarly reflects the effect of Ventilation = “Vent2” relative to the absence of ventilation. An odds ratio of 0.097 implies that the odds of the event are 90.3% lower for Ventilation = “Vent2” compared to the absence of ventilation.\nVentilationVent3: The odds ratio of 0.196 implies an 80.4% reduction in the odds of the event for Ventilation = “Vent3” compared to the absence of ventilation.\n\nThis interpretation clarifies that the intercept represents the starting point or baseline when there is no ventilation.\nWe can also visualize the predicted probabilities for each ventilation program as follows:\n\n# Predicted probabilities\npredicted_probs &lt;- predict(binomial_model_logit, type = \"response\", newdata = animal_data)\n\n# Visualize the predicted probabilities\nanimal_data$Predicted_Prob &lt;- predicted_probs\nggplot(animal_data, aes(x = Ventilation, y = Predicted_Prob)) +\n  geom_col(position = \"dodge\", fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Predicted Probabilities of 'Standing struss' by Ventilation Program\",\n       x = \"Ventilation Program\",\n       y = \"Predicted Probability\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n\n\n\n\nBut, we can also apply Tukey’s HSD (Honestly Significant Difference) test to compare the ventilation programs and then add letters to the predicted probability plot to indicate statistically significant groups. Here’s how you can do that:\n\n# Load necessary libraries\nlibrary(agricolae)\n\nWarning: package 'agricolae' was built under R version 4.3.2\n\n# Perform Tukey's LSD test\ntukey_results &lt;- LSD.test(aov(binomial_model_logit), \"Ventilation\", group = TRUE)\n\ntukey_results\n\n$statistics\n    MSerror Df Mean       CV  t.value       LSD\n  0.2233333 96  0.5 94.51631 1.984984 0.2653254\n\n$parameters\n        test p.ajusted      name.t ntr alpha\n  Fisher-LSD      none Ventilation   4  0.05\n\n$means\n        Behavior       std  r         se       LCL       UCL Min Max Q25 Q50\nControl     0.80 0.4082483 25 0.09451631 0.6123866 0.9876134   0   1   1   1\nVent1       0.48 0.5099020 25 0.09451631 0.2923866 0.6676134   0   1   0   0\nVent2       0.28 0.4582576 25 0.09451631 0.0923866 0.4676134   0   1   0   0\nVent3       0.44 0.5066228 25 0.09451631 0.2523866 0.6276134   0   1   0   0\n        Q75\nControl   1\nVent1     1\nVent2     1\nVent3     1\n\n$comparison\nNULL\n\n$groups\n        Behavior groups\nControl     0.80      a\nVent1       0.48      b\nVent3       0.44      b\nVent2       0.28      b\n\nattr(,\"class\")\n[1] \"group\"\n\n\n\n# Extract the letter results into a data frame\ntukey_g &lt;- as.data.frame(tukey_results$groups)\n\n# Extract the mean differences and group names\ntukey_df &lt;- as.data.frame(tukey_results$means)\ntukey_df &lt;- mutate(tukey_df, trt = rownames(tukey_df))\nmerged &lt;- left_join(tukey_df, tukey_g, by = \"Behavior\")\n\n\n# Visualize the predicted probabilities with Tukey's letters\nggplot(merged, aes(x = trt, y = Behavior, fill = trt)) +\n  geom_col(position = \"dodge\", color = \"black\") +\n  geom_errorbar(aes(ymin = LCL, ymax = UCL), width = 0.2) + \n  labs(title = \"Predicted Probabilities with Tukey's Letters\",\n       x = \"Ventilation Program\",\n       y = \"Predicted Probability\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  geom_text(data = merged, aes(x = trt, y = UCL + 0.05, label = groups), size = 4, color = \"red\") + \n  theme(legend.position = \"none\")\n\n\n\n\nIt’s crucial to highlight that the initial approach holds substantial advantages over employing post hoc tests. This is because the coefficients, significantly differing from zero, inherently indicate distinctions among all treatments. This fundamental insight simplifies the analysis, rendering post hoc tests less necessary in this context.\nAdditionally, the interpretability of log odds contributes to the straightforward understanding of the results. Unlike complex statistical measures, log odds provide a clear and accessible way to grasp the differences between treatments. This simplicity enhances the practical utility of the analysis, making it more accessible to a broader audience."
  },
  {
    "objectID": "dist.html#some-useful-distributions",
    "href": "dist.html#some-useful-distributions",
    "title": "8  GLM models framework",
    "section": "8.1 Some useful distributions",
    "text": "8.1 Some useful distributions\nIn experiments with dairy cows, statistical distributions, part of the exponential family, are essential for effectively modeling and interpreting various environmental and behavioral variables. Let’s explore an overview of the three main distributions and their relevance to the dairy cow experiment.\n\nExponential Distribution\nThe probability density function (PDF) of the exponential distribution is given by:\n\\[f(x | \\lambda) = \\lambda \\cdot e^{-\\lambda x} \\text{ for } x \\geq 0\\]\nwhere \\(\\lambda\\) is the rate parameter.\nGraphical Representation in R:\n\nlibrary(tidyverse)\n\nlambda &lt;- 0.5\nx &lt;- seq(0, 5, by = 0.1)\ny &lt;- lambda * exp(-lambda * x)\n\ndf_exp &lt;- data.frame(x, y)\nggplot(df_exp, aes(x, y)) +\n  geom_line(color = \"blue\", linewidth = 2) +\n  labs(title = \"Exponential Distribution\", x = \"x\", y = \"Probability Density\") +\n  annotate(\"text\", x = 3, y = 0.3, label = expression(lambda == 0.5), \n           color = \"blue\", size = 6) +\n  labs(title = \"Exponential Distribution\",\n       x = \"x\", \n       y = \"Probability Density\") +\n  theme_minimal()\n\n\n\n\n\n\nBinomial Distribution\nThe binomial distribution can be derived from the exponential distribution by considering the waiting time until the first success in a sequence of independent Bernoulli trials.\nThe probability function of the binomial distribution is given by:\n\\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nwhere \\(n\\) is the number of trials, \\(p\\) is the probability of success, and \\(k\\) is the number of successes.\nGraphical Representation in R:\n\nn &lt;- 10\np &lt;- 0.3\nx &lt;- seq(0, n, by = 1)\ny &lt;- dbinom(x, size = n, prob = p)\n\ndf_binom &lt;- data.frame(x, y)\nggplot(df_binom, aes(x, y)) +\n  geom_point(color = \"darkgreen\", size = 3) +\n  labs(title = \"Binomial Distribution\", x = \"Number of Successes\", y = \"Probability\") +\n  annotate(\"text\", x = 5, y = 0.2, label = expression(p == 0.3), \n           color = \"darkgreen\", size = 6) +\n  labs(title = \"Binomial Distribution\", \n       x = \"Number of Successes\", \n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\nGeneral Application: Modeling binary outcomes and ordinal categorical variables in the context of ordinal logistic regression.\nOrdinal Logistic Regression:\nOrdinal logistic regression is an extension of binomial logistic regression, designed to model ordinal outcomes with a meaningful order. The model introduces separate intercepts for each category, assuming the cumulative logits of each category are proportional.\nApplication: Valuable for analyzing data with ordered categorical responses, such as lameness scores in dairy cows.\n\n\nPoisson Distribution\nThe Poisson distribution can be derived from the exponential distribution by considering the number of events in fixed intervals of time or space.\nThe probability function of the poison distribution is given by: \\[P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\]\nwhere \\(\\lambda\\) is the average rate of events.\nGraphical Representation in R:\n\nlambda &lt;- 3\nx &lt;- seq(0, 10, by = 1)\ny &lt;- dpois(x, lambda)\n\ndf_poisson &lt;- data.frame(x, y)\nggplot(df_poisson, aes(x, y)) +\n  geom_point(color = \"orange\", size = 3) +\n  labs(title = \"Poisson Distribution\", x = \"Number of Events\", y = \"Probability\") +\n  annotate(\"text\", x = 7, y = 0.15, label = expression(lambda == 3), \n           color = \"orange\", size = 6) +\n  ggtitle(\"Poisson Distribution\") +\n  labs(x = \"Number of Events\", y = \"Probability\") +\n  theme_minimal()\n\n\n\n\nGeneral Application: Suitable for count data, especially in scenarios involving rare events.\n\n\nGaussian Distribution\nThe Gaussian distribution can be obtained as a limiting case of the binomial distribution when the number of trials becomes large. The Central Limit Theorem plays a crucial role in this transition.\nThe probability function density function of the binomial distribution is given by:\n\\[f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.\nGraphical Representation in R:\n\nmu &lt;- 0\nsigma &lt;- 1\nx &lt;- seq(-5, 5, by = 0.1)\ny &lt;- dnorm(x, mean = mu, sd = sigma)\n\ndf_gaussian &lt;- data.frame(x, y)\nggplot(df_gaussian, aes(x, y)) +\n  geom_line(color = \"red\", linewidth = 2) +\n  labs(title = \"Gaussian Distribution\", x = \"x\", y = \"Probability Density\") +\n  annotate(\"text\", x = -3, y = 0.35, label = expression(mu == 0 ~ and ~ sigma == 1), \n           color = \"red\", size = 6) +\n  ggtitle(\"Gaussian Distribution\") +\n  labs(x = \"x\", y = \"Probability Density\") +\n  theme_minimal()\n\n\n\n\nGeneral Application: Modeling continuous variables, offering a versatile framework for various measurements in diverse fields."
  },
  {
    "objectID": "dist.html#the-glm-framework-in-r",
    "href": "dist.html#the-glm-framework-in-r",
    "title": "8  GLM models framework",
    "section": "8.2 The GLM framework in R",
    "text": "8.2 The GLM framework in R\nAs previously discussed, Generalized Linear Models (GLMs) provide a flexible framework for modeling various data types, going beyond traditional linear regression. The three key components of a GLM include the distribution of the response variable, the linear predictor, and the link function connecting the linear predictor to the expected value of the response.\nNow, let’s explore how to define these components in the context of the earlier mentioned distributions, using R.\n\nBinomial models\n\n# Example: Modeling the probability of success in binary outcomes\nglm_binomial &lt;- glm(response_binary ~ predictor, family = binomial(link = \"logit\"))\n\nHere, we use the binomial family to specify the distribution and the logit link function, which connects the linear predictor to the probability of success in binary outcomes.\n\n\nPoisson models\n\n# Example: Modeling count data with the Poisson distribution\nglm_poisson &lt;- glm(response_count ~ predictor, family = poisson(link = \"log\"))\n\nIn this case, the poisson family is chosen for count data, and the log link function is applied.\n\n\nOrdinal models\n\n# Assuming 'categories' is a factor with ordered levels: \"Low (1)\", \"Medium(2)\", \"High(3)\"\nglm_ordinal &lt;- glm(categories ~ predictor, family = cumulative(link = \"logit\"))\n\nIn this example, the cumulative family is used for ordinal logistic regression within the glm function. The link argument specifies the logit link function, connecting the linear predictor to the cumulative odds of being in or below each category.\n\n\nGaussian models\n\n# Example: Modeling continuous variables with Gaussian distribution\nglm_gaussian &lt;- glm(response_continuous ~ predictor, family = gaussian(link = \"identity\"))\n\nContinuous variables can be modeled using the gaussian family, and the choice between the identity or log link function allows for the establishment of a linear relationship between the predictor and the expected value of the response. Opting for the identity function results in a model that closely mirrors the one introduced in chapter 6.\n\n\nGeneral framework\nIn each example, the glm function is utilized to fit the model, and the family argument specifies the distribution family, while the link argument defines the link function. These GLM components offer flexibility in adapting the model to diverse data types, providing a powerful tool for statistical modeling in R. It’s noteworthy that beyond the linking functions illustrated in prior examples, there are numerous others available.\nThe following table summarize the association of each distribution with its corresponding linking function and the most commom type of variables used in dairy cows experiment:\n\nCorrespondence between each probability distribution, its associated linking functions, and the most common outcomes measured in dairy cow experiments.\n\n\n\n\n\n\n\nDistribution\nLinking Function\nVariables\n\n\n\n\nBinomial\nLogit, Probit, cloglog\nAbsence and occurrence of behaviors\n\n\nPoisson\nLog\nNumber of behaviors per time interval, heart rate\n\n\nOrdinal\nLogit\nLameness score\n\n\nGaussian\nIdentity, Log\nMilk production, humidity, temperature, etc.\n\n\n\nThe table offers a quick guide for selecting relevant distributions, linking functions, and understanding variables in dairy cow behavior studies. In the next chapter, we will further explore the application of these four models in dairy cow experiments."
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "9  GLM Analysis: A Practical Application",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "example.html#overview",
    "href": "example.html#overview",
    "title": "9  GLM Analysis: A Practical Application",
    "section": "9.1 Overview",
    "text": "9.1 Overview\nThis final section focuses on the practical application of statistical modeling techniques using data from the 2019 study by Pilatti et al. The study investigated diurnal behaviors, thermoregulatory variables, and lameness scores of dairy cows in a compost-bedded pack barn system under hot and humid conditions Pilatti et al. (2019).\n\n9.1.1 Data Overview\nThe dataset, including diurnal behaviors, thermoregulatory variables, and lameness scores, is accessible for practical application of generalized linear models (GLM). This enables an analysis of the impact of social groups on these three variables.\n\n\n9.1.2 GitHub Repository\nAccess the dataset on our GitHub repository: GitHub Repository.\nIn the following sections, we guide you through applying GLM techniques to analyze the influence of different factors on diurnal behaviors, thermoregulatory variables, and lameness scores."
  },
  {
    "objectID": "example.html#analysis",
    "href": "example.html#analysis",
    "title": "9  GLM Analysis: A Practical Application",
    "section": "9.2 Analysis",
    "text": "9.2 Analysis\n\n9.2.1 Install and Load Libraries\nTo start the analysis, we need to install and load the necessary R libraries. Execute the following commands to ensure you have the required tools at your disposal:\n\n# Install and load necessary libraries\n#install.packages(c(\"tidyverse\", \"readxl\", \"skimr\"))\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\n\n\n\n9.2.2 Import amd manipulate datasets\nNow, let’s import the dataset into R for exploration and analysis. The dataset is available on our GitHub repository. Use the following code to import the data:\n\n# Load the dataset from Excel\nexcel_path &lt;- \"data/Dados_Pillati_2018.xlsx\"\ndf_bh &lt;- read_excel(excel_path, sheet = \"COMPORTAMENTO\")\ndf_term &lt;- read_excel(excel_path, sheet = \"TERMORREG\")\ndf_lam &lt;- read_excel(excel_path, sheet = \"BEA\")\n\n#Transform cathegorical variables to factors\n\ndf_bh &lt;- df_bh |&gt; \n  mutate_at(vars(DIA, TRAT, ANIMAL), as.factor) \n\ndf_term &lt;- df_term |&gt; \n  mutate_at(vars(DIA, TRAT, ANIMAL), as.factor) \n\ndf_lam &lt;- df_lam |&gt; \n  mutate_at(vars(DIA, TRAT, ANIMAL), as.factor) |&gt;\n  mutate_at(vars(HIG, CLAUD), ~factor(., levels = c(\"0\",\"1\",\"2\",\"3\",\"4\")))\n\n\n\n9.2.3 Animal Behaviors Analysis\n\n9.2.3.1 Dataset Overview\n\n# Display a summary of the diurnal behaviors dataset\nskimr::skim(df_bh)\n\n\nData summary\n\n\nName\ndf_bh\n\n\nNumber of rows\n4032\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDIA\n0\n1\nFALSE\n24\n1: 168, 2: 168, 3: 168, 4: 168\n\n\nTRAT\n0\n1\nFALSE\n2\n1: 2016, 2: 2016\n\n\nANIMAL\n0\n1\nFALSE\n12\n1: 336, 2: 336, 3: 336, 4: 336\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHORA\n0\n1.00\n7.50\n4.03\n1\n4\n7.5\n11\n14\n▇▇▅▇▇\n\n\nOP\n230\n0.94\n0.40\n0.49\n0\n0\n0.0\n1\n1\n▇▁▁▁▆\n\n\nOD\n225\n0.94\n0.30\n0.49\n0\n0\n0.0\n1\n11\n▇▁▁▁▁\n\n\nRP\n224\n0.94\n0.20\n0.40\n0\n0\n0.0\n0\n1\n▇▁▁▁▂\n\n\nRD\n227\n0.94\n0.22\n0.41\n0\n0\n0.0\n0\n1\n▇▁▁▁▂\n\n\nC\n231\n0.94\n0.33\n0.47\n0\n0\n0.0\n1\n1\n▇▁▁▁▃\n\n\nIA\n225\n0.94\n0.09\n0.28\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nAN\n225\n0.94\n0.17\n0.37\n0\n0\n0.0\n0\n1\n▇▁▁▁▂\n\n\nOF\n225\n0.94\n0.01\n0.11\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nLA\n224\n0.94\n0.12\n0.33\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nCO\n224\n0.94\n0.08\n0.27\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nAF\n224\n0.94\n0.03\n0.16\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nEM\n224\n0.94\n0.02\n0.12\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nCA\n224\n0.94\n0.03\n0.17\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nL\n224\n0.94\n0.02\n0.14\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nM\n225\n0.94\n0.00\n0.07\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nBC\n224\n0.94\n0.03\n0.16\n0\n0\n0.0\n0\n1\n▇▁▁▁▁\n\n\n\n\n\nBrief Description od data: The behaviors dataset provides an overview of the observed behaviors, allowing us to understand the patterns and trends in the daily activities of dairy cows, categorized into two social groups: primiparous and multiparous, within a compost barn setting.\n\n\nModel definition\nWe opt for a binomial Generalized Linear Model (GLM) with a logit linking function to model behavior. This choice is suitable because our response variable involves the occurrence or non-occurrence of a certain behavior at a given moment in time, resulting in a dichotomous outcome—either the behavior is present or absent.\nAlternatively, if our focus were on quantifying the number of times a particular behavior occurs within a specified time frame, a Poisson GLM with a log link function would be more appropriate. This model is well-suited for count data, where the response variable represents the number of events (occurrences of behavior in this context) and follows a Poisson distribution.\nAs we measure behavior throughout daylight hours for both social groups—primiparous and multiparous cows—our primary interest lies in comparing these groups along the course of the day. The binomial GLM with a logit link will help us assess the probability of behavior occurrence at different times, shedding light on potential variations in behavior patterns between the two social groups during daylight hours. Certainly! Here’s a continuation, starting with the mathematical description of the binomial GLM model:\nThus, our model is expressed mathematically as follows:\n\\[\\text{log}\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\text{HORA} + \\beta_2 \\text{TRAT} + \\beta_3 \\text{HORA} \\times \\text{TRAT}\\]\nwhere:\n\n\\(p\\) is the probability of behavior occurrence,\n\\(\\text{HORA}\\) represents the time of day,\n\\(\\text{TRAT}\\) denotes the treatment or social group (primiparous and multiparous), and\n\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)) are the model coefficients.\n\nIn R, this model can be fitted using the glm function:\n\nmod_op &lt;- glm(OP ~ HORA * TRAT, \n              family = binomial(link = \"logit\"),\n              data = df_bh)\n\nThe variable OP denotes the binary outcome, indicating the occurrence (1) or non-occurrence (0) of the behavior under consideration. Meanwhile, HORA, TRAT, and DIA serve as predictor variables, with DIA specifically treated as a control variable.\nThe glm function in R maximizes the likelihood of the observed data under the specified binomial distribution with a logit link, providing estimates for the model coefficients.\n\nsummary(mod_op)\n\n\nCall:\nglm(formula = OP ~ HORA * TRAT, family = binomial(link = \"logit\"), \n    data = df_bh)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.57683    0.10320  -5.589 2.28e-08 ***\nHORA         0.01592    0.01205   1.321  0.18639    \nTRAT2        0.42108    0.14031   3.001  0.00269 ** \nHORA:TRAT2  -0.04132    0.01648  -2.507  0.01219 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5123.4  on 3801  degrees of freedom\nResidual deviance: 5113.7  on 3798  degrees of freedom\n  (230 observations deleted due to missingness)\nAIC: 5121.7\n\nNumber of Fisher Scoring iterations: 4\n\n\nSimilar to linear models, the assessment of predictor effects in Generalized Linear Models (GLMs) can be conducted through an analysis of deviance table and chi-square tests. This can be achieved using the Anova function from the car package. The following code exemplifies the procedure:\n\n# Install and load the necessary library\n#install.packages(\"car\")\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n# Perform analysis of deviance table and chi-square test\nAnova(mod_op, type = \"II\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: OP\n          LR Chisq Df Pr(&gt;Chisq)  \nHORA        0.5659  1    0.45190  \nTRAT        2.8337  1    0.09231 .\nHORA:TRAT   6.2930  1    0.01212 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis approach facilitates the examination of the significance of individual predictors in the model. The type = \"II\" argument specifies the use of Type II sums of squares in the analysis. Further details on the method and interpretation can be found Dobson and Barnett (2008) and in the documentation for the car package (Fox and Weisberg (2020)).\nThe examination of the table reveals a notable interaction between the variables HORA and TRAT. Subsequently, we aim to objectively visualize predictions, along with prediction intervals, to assess the behavior probabilities of both social groups across varying time points:\n\nlibrary(ggplot2)\n\n# Create a data frame for prediction\npred_data &lt;- expand.grid(\n  TRAT = unique(df_bh$TRAT),\n  HORA = seq(min(df_bh$HORA), max(df_bh$HORA), length.out = 100))\n\n# Make predictions\npred_probs &lt;- predict(mod_op, newdata = pred_data, type = \"response\", se.fit = TRUE)\n\n# Combine predictions and intervals with prediction data\npred_data &lt;- cbind(pred_data, as.data.frame(pred_probs))\n\n\n# Plot using ggplot2\nggplot(pred_data, aes(x = HORA, y = fit, color = TRAT, fill = TRAT)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = fit - 1.96 * se.fit, ymax = fit + 1.96 * se.fit, fill = TRAT), alpha = 0.2) +\n  labs(title = \"Probability of 'OP' Behavior across Day time\",\n       x = \"Time of Day\",\n       y = \"Probability of Behavior\") +\n  scale_x_continuous(breaks = seq(1,14,1), labels = c(\"08:00\", \"08:40\", \"09:20\", \"10:00\", \"10:40\", \"11:20\", \"12:00\", \"12:40\", \"13:20\", \"14:00\", \"14:40\",    \"15:20\", \"16:00\",   \"16:40\")) +\n  scale_color_discrete(name = \"Social Group\") +\n  scale_fill_discrete(name = \"Social Group\") +\n  theme_minimal()\n\n\n\n\nHowever, should the consideration of the DIA variable be omitted? Let’s investigate.\nWe can extend the model by incorporating DIA as a random effect using a mixed-effects model. To achieve this, we’ll utilize the glmer function from the lme4 package:\n\n# Install and load necessary libraries\n#install.packages(\"lme4\")\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n# Fit the mixed-effects model with DAY as a random effect\nmod_op_mixed &lt;- glmer(OP ~ HORA * TRAT + (1 | DIA), \n                   family = binomial(link = \"logit\"), \n                   data = df_bh)\n\n# Print model summary\nsummary(mod_op_mixed)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: OP ~ HORA * TRAT + (1 | DIA)\n   Data: df_bh\n\n     AIC      BIC   logLik deviance df.resid \n  5085.4   5116.6  -2537.7   5075.4     3797 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.3469 -0.8167 -0.7139  1.1604  1.6226 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n DIA    (Intercept) 0.07969  0.2823  \nNumber of obs: 3802, groups:  DIA, 24\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.58801    0.11908  -4.938 7.89e-07 ***\nHORA         0.01622    0.01216   1.334  0.18225    \nTRAT2        0.43087    0.14165   3.042  0.00235 ** \nHORA:TRAT2  -0.04215    0.01664  -2.533  0.01131 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) HORA   TRAT2 \nHORA       -0.772              \nTRAT2      -0.644  0.649       \nHORA:TRAT2  0.564 -0.731 -0.881\n\n\nIn this model, (1 | DIA) specifies that DIA is a random effect. The (1 | DIA) term assumes a random intercept for each level of DAY.\nLet’s perform a model comparison using the Akaike Information Criterion (AIC), a widely used metric for model selection (Burnham and Anderson (2002)):\n\n# Compare models using AIC\nAIC_comparison &lt;- AIC(mod_op, mod_op_mixed)\n\nChoosing a model based on the Akaike Information Criterion (AIC) involves selecting the model with the lowest AIC value among a set of candidate models. AIC is a measure of relative model fit that balances the goodness-of-fit with the complexity of the model (Burnham and Anderson (2004)).\nIt seems that the model, including DIA as a random term, is more plausible. However, analytical computation of standard errors and intervals for mixed-effects models is not straightforward. To address this, we need to employ numerical approximation methods, often through resampling techniques. However, this course does not emphasize these advanced techniques. Instead, we will acknowledge that daily variation is a natural fluctuation inherent in the experiment, leading to larger prediction intervals.\n\n\n\n9.2.4 Thermoregulatory Variables Analysis\n\n9.2.4.1 Dataset Overview:\n\n# Display a summary of the thermoregulatory variables dataset\nskimr::skim(df_term)\n\n\nData summary\n\n\nName\ndf_term\n\n\nNumber of rows\n864\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDIA\n0\n1\nFALSE\n24\n1: 36, 2: 36, 3: 36, 4: 36\n\n\nTRAT\n0\n1\nFALSE\n2\n1: 433, 2: 431\n\n\nANIMAL\n0\n1\nFALSE\n12\n1: 72, 2: 72, 3: 72, 4: 72\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHORA\n0\n1.00\n2.00\n0.82\n1.00\n1.00\n2.00\n3.00\n3.00\n▇▁▇▁▇\n\n\nTF\n48\n0.94\n28.48\n3.28\n15.20\n26.60\n28.60\n31.00\n35.60\n▁▁▅▇▃\n\n\nTP\n48\n0.94\n31.30\n2.23\n21.20\n30.00\n31.60\n33.00\n35.60\n▁▁▃▇▅\n\n\nTC\n48\n0.94\n31.35\n2.30\n22.00\n29.95\n31.60\n33.00\n36.60\n▁▁▅▇▂\n\n\nTFL\n48\n0.94\n31.47\n2.56\n3.60\n30.00\n32.00\n33.20\n39.60\n▁▁▁▇▅\n\n\nTCX\n48\n0.94\n29.98\n2.64\n20.00\n28.40\n30.20\n32.00\n35.80\n▁▂▆▇▂\n\n\nTMS\n48\n0.94\n30.51\n2.40\n21.88\n29.04\n30.82\n32.32\n35.32\n▁▂▅▇▃\n\n\nFR15\n48\n0.94\n13.94\n5.30\n4.00\n10.00\n13.00\n18.00\n33.00\n▅▇▅▂▁\n\n\nFR\n49\n0.94\n55.74\n21.22\n16.00\n40.00\n52.00\n72.00\n132.00\n▅▇▅▂▁\n\n\n\n\n\nBrief Description: The thermoregulatory variables dataset encompasses information on various physiological measures, providing insights into how two distinct social groups of dairy cows regulate their body temperature within a compost barn setting.\nHere, we will work with two variables, one being continuous (superficial temperature) and the other discrete (respiratory rate). Since temperature is continuous, it is expected to have a normal distribution, while respiratory rate, being obtained by counting, is expected to have a Poisson distribution.\n\n\nModel definition for temperature\nWe could initially employ the classical linear model for superficial temperature, incorporating Gaussian errors and applying the least squares method. Nevertheless, for the sake of consistency, we will adopt its equivalent Generalized Linear Model (GLM) in the following manner:\n\\[Y_{ij} = \\beta_0 + \\beta_1 \\text{HORA} + \\beta_2 \\text{TRAT} + \\beta_3 \\text{HORA} \\times \\text{TRAT} + \\epsilon_{ij}\\]\nwhere:\n\n\\(Y_ijk\\) is the temperature of \\(i\\)-th animal of \\(j\\)-th social group measured at \\(k\\)-th day time,\n\\(\\text{HORA}\\) represents the time of day,\n\\(\\text{TRAT}\\) denotes the treatment or social group (primiparous and multiparous),\n(\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\beta_3\\)) are the model coefficients, and\n\\(\\epsilon_ij\\) is the normal distributed error term.\n\nIn R, this model can be fitted using the glm function:\n\nmod_temp &lt;- glm(TMS ~ HORA * TRAT, \n              family = gaussian(link = \"identity\"),\n              data = df_term)\n\nTo view the model coefficients:\n\nsummary(mod_temp)\n\n\nCall:\nglm(formula = TMS ~ HORA * TRAT, family = gaussian(link = \"identity\"), \n    data = df_term)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.83994    0.30415  94.822  &lt; 2e-16 ***\nHORA         0.96822    0.14102   6.866 1.31e-11 ***\nTRAT2       -0.55089    0.41797  -1.318    0.188    \nHORA:TRAT2   0.02199    0.19363   0.114    0.910    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5.070756)\n\n    Null deviance: 4690.6  on 815  degrees of freedom\nResidual deviance: 4117.5  on 812  degrees of freedom\n  (48 observations deleted due to missingness)\nAIC: 3646.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nAnd applying type II Analysis of deviance:\n\ncar::Anova(mod_temp, type = \"II\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: TMS\n          LR Chisq Df Pr(&gt;Chisq)    \nHORA       102.819  1  &lt; 2.2e-16 ***\nTRAT        10.306  1   0.001326 ** \nHORA:TRAT    0.013  1   0.909592    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Analysis of Deviance Table indicates that the variable HORA significantly influences the response variable TMS (p &lt; 0.001), while TRAT and the interaction term HORA:TRAT do not show statistically significant effects (p &gt; 0.05). This underscores the prominent role of “HORA” in explaining variability in TMS within the model.\nLet´s see the results graphically:\n\n# Create a data frame for prediction\npred_data &lt;- expand.grid(\n  TRAT = unique(df_term$TRAT),\n  HORA = seq(min(df_term$HORA), max(df_term$HORA), length.out = 100))\n\n\n# Make predictions\npred_probs_temp &lt;- predict(mod_temp, newdata = pred_data, type = \"response\", se.fit = TRUE)\n\n# Combine predictions and intervals with prediction data\npred_data_temp &lt;- cbind(pred_data, as.data.frame(pred_probs_temp))\n\n\n# Plot using ggplot2\nggplot(pred_data_temp, aes(x = HORA, y = fit, color = TRAT, fill = TRAT)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = fit - 1.96 * se.fit, ymax = fit + 1.96 * se.fit, fill = TRAT), alpha = 0.2) +\n  labs(title = \"Animal superficial temperature across Day time\",\n       x = \"Time of Day\",\n       y = \"Superficial temperature\") +\n  scale_x_continuous(breaks = seq(1,3,1), labels = c(\"08:00\", \"08:40\",  \"09:20\")) +\n  scale_color_discrete(name = \"Social Group\") +\n  scale_fill_discrete(name = \"Social Group\") +\n  theme_minimal()\n\n\n\n\n\n\nModel definition for respiratory frequency\nTo model the respiratory frequency (FR) using a Poisson distribution with a log link function, the relationship is expressed as follows:\n\\[\\log(\\lambda_{ijk}) = \\beta_0 + \\beta_1 \\text{HORA} + \\beta_2 \\text{TRAT} + \\beta_3 \\text{HORA} \\times \\text{TRAT}\\]\nwhere:\n\n\\(\\log(\\lambda_{ijk})\\) is the natural logarithm of the expected count of respiratory frequency for the \\(i\\)-th animal of the \\(j\\)-th social group measured at the \\(k\\)-th time of day,\n\\(\\text{HORA}\\) represents the time of day,\n\\(\\text{TRAT}\\) denotes the treatment or social group (primiparous and multiparous),\n\\(\\beta_0, \\beta_1, \\beta_2,\\) and \\(\\beta_3\\) are the model coefficients.\n\nIn R, this model can be fitted using the glm function with a Poisson distribution and a log link:\n\n# Model definition for respiratory frequency (FR)\nmod_fr &lt;- glm(FR ~ HORA * TRAT, \n              family = poisson(link = \"log\"),\n              data = df_term)\n\nTo view the model coefficients:\n\nsummary(mod_fr)\n\n\nCall:\nglm(formula = FR ~ HORA * TRAT, family = poisson(link = \"log\"), \n    data = df_term)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.587264   0.019198 186.855   &lt;2e-16 ***\nHORA         0.224335   0.008374  26.789   &lt;2e-16 ***\nTRAT2       -0.012572   0.026635  -0.472   0.6369    \nHORA:TRAT2  -0.022632   0.011644  -1.944   0.0519 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6401.5  on 814  degrees of freedom\nResidual deviance: 5005.0  on 811  degrees of freedom\n  (49 observations deleted due to missingness)\nAIC: 9731.2\n\nNumber of Fisher Scoring iterations: 4\n\n\nAnd applying Type II Analysis of Deviance:\n\ncar::Anova(mod_fr, type = \"II\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: FR\n          LR Chisq Df Pr(&gt;Chisq)    \nHORA       1350.75  1  &lt; 2.2e-16 ***\nTRAT         42.22  1  8.141e-11 ***\nHORA:TRAT     3.78  1    0.05192 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Analysis of Deviance Table indicates that time of day significantly influences respiratory frequency. However, the social group TRAT2 alone does not show a significant effect on FR, and the interaction term HORA:TRAT2 exhibits borderline significance.\nSubsequently, the graphical representation illustrates the predicted FR with confidence intervals across different times of the day and social groups.\n\n# Create a data frame for prediction\npred_data_fr &lt;- expand.grid(\n  TRAT = unique(df_term$TRAT),\n  HORA = seq(min(df_term$HORA), max(df_term$HORA), length.out = 100))\n\n# Make predictions\npred_probs_fr &lt;- predict(mod_fr, newdata = pred_data_fr, type = \"response\", se.fit = TRUE)\n\n# Combine predictions and intervals with prediction data\npred_data_fr &lt;- cbind(pred_data_fr, as.data.frame(pred_probs_fr))\n\n# Plot using ggplot2\nggplot(pred_data_fr, aes(x = HORA, y = fit, color = TRAT, fill = TRAT)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = fit - 1.96 * se.fit, ymax = fit + 1.96 * se.fit, fill = TRAT), alpha = 0.2) +\n  labs(title = \"Animal respiratory frequency across Day time\",\n       x = \"Time of Day\",\n       y = \"Respiratory Frequency\") +\n  scale_x_continuous(breaks = seq(1, 3, 1), labels = c(\"08:00\", \"08:40\", \"09:20\")) +\n  scale_color_discrete(name = \"Social Group\") +\n  scale_fill_discrete(name = \"Social Group\") +\n  theme_minimal()\n\n\n\n\n\n\n\n9.2.5 Lameness Scores Analysis\n\nDataset Overview\n\n# Display a summary of the lameness scores dataset\nskimr::skim(df_lam)\n\n\nData summary\n\n\nName\ndf_lam\n\n\nNumber of rows\n144\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDIA\n0\n1.00\nFALSE\n12\n1: 12, 2: 12, 3: 12, 4: 12\n\n\nTRAT\n0\n1.00\nFALSE\n2\n1: 72, 2: 72\n\n\nANIMAL\n0\n1.00\nFALSE\n12\n1: 12, 2: 12, 3: 12, 4: 12\n\n\nCLAUD\n8\n0.94\nFALSE\n4\n1: 81, 2: 51, 3: 3, 4: 1\n\n\nHIG\n8\n0.94\nFALSE\n3\n1: 91, 2: 33, 3: 12, 0: 0\n\n\n\n\n\nBrief Description: The lameness scores dataset documents the degree of lameness in dairy cows, presenting an opportunity to investigate whether social hierarchy plays a role in influencing lameness under specific environmental conditions, such as in a compost barn.\n\n\n9.2.5.1 Model definition\nIn this analysis, we explore the application of cumulative ordinal regression to examine the lameness scores of dairy cows across multiple days within distinct social groups.\nThe cumulative ordinal regression model represents an extension of the binomial logistic regression model, accommodating situations where the response variable is ordinal. In the context of our study, lameness scores are not merely binary (present or absent), but rather fall into multiple ordered categories. This model enables us to explore the relationship between predictor variables, such as days and social groups, and the ordinal nature of the lameness scores.\nOur cumulative ordinal regression model can be expressed as follows:\n\\[P(Y \\leq j) = F(\\alpha_j + \\beta_1 \\cdot TRAT\\]\nwhere:\n\n\\(P(Y \\leq j)\\) is the cumulative probability that the lameness score falls into or below category \\(j\\),\n\\(F()\\) is the cumulative distribution function corresponding to the chosen link function,\n\\(\\alpha_j\\) represents the threshold parameter associated with category \\(j\\),\n\\(\\beta_1\\) is the coefficient for the predictor variable \\(TRAT\\),\n\nTo fit the model we can use the function polr of package MASS, as follows:\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n## fit ordered logit model and store results 'm'\nm_claud &lt;- polr(CLAUD ~  TRAT, data = df_lam, Hess=TRUE)\n\n## view a summary of the model\nsummary(m_claud)\n\nCall:\npolr(formula = CLAUD ~ TRAT, data = df_lam, Hess = TRUE)\n\nCoefficients:\n       Value Std. Error t value\nTRAT2 -1.299     0.3665  -3.544\n\nIntercepts:\n    Value    Std. Error t value \n0|1 -12.3341  25.5288    -0.4831\n1|2  -0.2400   0.2462    -0.9747\n2|3   3.0204   0.5208     5.8000\n3|4   4.4296   1.0128     4.3736\n\nResidual Deviance: 203.4066 \nAIC: 213.4066 \n(8 observations deleted due to missingness)\n\n\nThere is no significance test available for TRAT. Instead, we can inspect the confidence interval to determine significance by checking whether it encompasses zero:\n\n(ci &lt;- confint(m_claud)) # computing confidence interval (CI)\n\nWaiting for profiling to be done...\n\n\n     2.5 %     97.5 % \n-2.0352746 -0.5935379 \n\n\nUnderstanding the coefficients of the model might pose a challenge as they are presented in log-scale. An alternative approach for interpreting logistic regression models involves converting these coefficients into odds ratios. By exponentiating both the estimates and confidence intervals, we can obtain the odds ratios along with their respective confidence intervals.\n\n## Exponentiate estimate and CI\nexp(coef(m_claud))\n\n   TRAT2 \n0.272872 \n\nexp(ci)\n\n    2.5 %    97.5 % \n0.1306446 0.5523696 \n\n\nThe interpretation is: For cows belonging to the social group “TRAT2” (primiparous), the odds of receiving higher scores of CLAUD (i.e., scores closest to 4) are 1-0.272872 = 0.727128 times lower (72.72% lower) compared to the multiparous cows, while holding all other variables constant.\nWe can also visualize the predicted probabilities of the scores within each social group as follows:\n\n# install.packages(\"effects\")\n\n# Load the 'effects' package for visualizing model effects\nlibrary(effects)\n\n# Predict probabilities using the model for the specified data frame 'df_lam'\nt_prob &lt;- as.data.frame(predict(object = m_claud, df_lam, type = \"p\"))\n\n# Reshape the predicted probabilities and the treatment variable 'TRAT' using melt\n# This step is useful for creating a tidy data format suitable for ggplot2\nprob &lt;- reshape2::melt(cbind(t_prob, df_lam$TRAT))\n\n# Rename the columns for clarity\nnames(prob) &lt;- c(\"TRAT\", \"CLAUD\", \"Probability\")\n\n# Create a ggplot to visualize the probabilities\nggplot(prob, aes(x = CLAUD, y = Probability, fill = TRAT)) + \n  labs(x = \"Scores\",y =\"Probability (%)\") +  # Label for x-axis an y-axis\n  geom_col() +  # Use geom_col to create a bar plot based on the melted data\n  scale_fill_discrete(name = \"Group\", labels = c(\"1\" = \"Multiparous\", \"2\" = \"Primiparous\"))"
  }
]